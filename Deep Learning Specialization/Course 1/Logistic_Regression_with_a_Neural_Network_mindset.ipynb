{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network mindset\n",
    "\n",
    "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "- Use `np.dot(X,Y)` to calculate dot products.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Overview of the Problem set](#2)\n",
    "    - [Exercise 1](#ex-1)\n",
    "    - [Exercise 2](#ex-2)\n",
    "- [3 - General Architecture of the learning algorithm](#3)\n",
    "- [4 - Building the parts of our algorithm](#4)\n",
    "    - [4.1 - Helper functions](#4-1)\n",
    "        - [Exercise 3 - sigmoid](#ex-3)\n",
    "    - [4.2 - Initializing parameters](#4-2)\n",
    "        - [Exercise 4 - initialize_with_zeros](#ex-4)\n",
    "    - [4.3 - Forward and Backward propagation](#4-3)\n",
    "        - [Exercise 5 - propagate](#ex-5)\n",
    "    - [4.4 - Optimization](#4-4)\n",
    "        - [Exercise 6 - optimize](#ex-6)\n",
    "        - [Exercise 7 - predict](#ex-7)\n",
    "- [5 - Merge all functions into a model](#5)\n",
    "    - [Exercise 8 - model](#ex-8)\n",
    "- [6 - Further analysis (optional/ungraded exercise)](#6)\n",
    "- [7 - Test with your own image (optional/ungraded exercise)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](https://numpy.org/doc/1.20/) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### v1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1\n",
    "Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921fe679a632ec7ec9963069fa405725",
     "grade": false,
     "grade_id": "cell-c4e7e9c1f174eb83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "#(≈ 3 lines of code)\n",
    "# m_train = \n",
    "# m_test = \n",
    "# num_px = \n",
    "# YOUR CODE STARTS HERE\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test  = test_set_x_orig.shape[0]\n",
    "num_px  = train_set_x_orig.shape[1]\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td> m_train </td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2\n",
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a2aa62bdd8c01450111b758ef159aec",
     "grade": false,
     "grade_id": "cell-0f43921062c34e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "#(≈ 2 lines of code)\n",
    "# train_set_x_flatten = ...\n",
    "# test_set_x_flatten = ...\n",
    "# YOUR CODE STARTS HERE\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten  = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "# Check that the first 10 pixels of the second image are in the correct place\n",
    "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> \n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "    \n",
    "**What you need to remember:**\n",
    "\n",
    "Common steps for pre-processing a new dataset are:\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - sigmoid\n",
    "Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "239ab1cf1028b721fd14f31b8103c40d",
     "grade": false,
     "grade_id": "cell-520521c430352f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 1 line of code)\n",
    "    # s = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1 / (1+np.exp(-z))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0483e6820669111a9c5914d8b24bc315",
     "grade": true,
     "grade_id": "cell-30ea3151cab9c491",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62245933 0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0, 2.0])\n",
    "output = sigmoid(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Initializing parameters\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - initialize_with_zeros\n",
    "Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a37e375a85ddab7274a33abf46bb7c",
     "grade": false,
     "grade_id": "cell-befa9335e479864e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 lines of code)\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.0\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c13b0eafa46ca94de21b41faea8c58",
     "grade": true,
     "grade_id": "cell-a3b6699f145f3a3f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0.0\n",
      "\u001b[92mFirst test passed!\n",
      "\u001b[92mSecond test passed!\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "\n",
    "assert type(b) == float\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "initialize_with_zeros_test_1(initialize_with_zeros)\n",
    "initialize_with_zeros_test_2(initialize_with_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - propagate\n",
    "Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8552b2c9cff2b5fa537fab9f98a6e4da",
     "grade": false,
     "grade_id": "cell-11af17e28077b3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    grads -- dictionary containing the gradients of the weights and bias\n",
    "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
    "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    #(≈ 2 lines of code)\n",
    "    # compute activation\n",
    "    # A = ...\n",
    "    # compute cost by using np.dot to perform multiplication. \n",
    "    # And don't use loops for the sum.\n",
    "    # cost = ...                                \n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z = np.dot(w.T , X) + b \n",
    "    A = sigmoid(Z)\n",
    "    cost = - (1/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    print(cost)\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dz = A - Y \n",
    "    db = np.sum(dz) / m \n",
    "    dw = np.dot(X, dz.T) / m \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89373f564dc33ce8a883a55a6ef72b56",
     "grade": true,
     "grade_id": "cell-d1594d75b61dd554",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15900537707692405\n",
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.12500404500439652\n",
      "cost = 0.15900537707692405\n",
      "2.0424567983978403\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "\n",
    "# X is using 3 examples, with 2 features each\n",
    "# Each example is stacked column-wise\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "propagate_test(propagate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "dw = [[ 0.25071532]\n",
    " [-0.06604096]]\n",
    "db = -0.1250040450043965\n",
    "cost = 0.15900537707692405\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - optimize\n",
    "Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d9b4c1a780bf141c8eb48e06cbb494",
     "grade": false,
     "grade_id": "cell-616d6883e807448d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # (≈ 1 lines of code)\n",
    "        # Cost and gradient calculation \n",
    "        # grads, cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        # w = ...\n",
    "        # b = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        w = w - learning_rate * dw \n",
    "        b = b - learning_rate * db \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65a5c90f86a990614156e41f64b4678",
     "grade": true,
     "grade_id": "cell-8e3d43fbb82a8901",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15900537707692405\n",
      "0.15826114678694678\n",
      "0.1575224128216089\n",
      "0.15678912659059693\n",
      "0.15606123993328513\n",
      "0.15533870511622366\n",
      "0.15462147483059352\n",
      "0.15390950218963298\n",
      "0.15320274072603426\n",
      "0.1525011443893159\n",
      "0.15180466754316785\n",
      "0.15111326496277544\n",
      "0.15042689183212055\n",
      "0.14974550374126339\n",
      "0.14906905668360454\n",
      "0.14839750705313143\n",
      "0.14773081164164753\n",
      "0.1470689276359884\n",
      "0.14641181261522332\n",
      "0.14575942454784657\n",
      "0.14511172178895676\n",
      "0.14446866307742756\n",
      "0.14383020753307035\n",
      "0.1431963146537888\n",
      "0.14256694431272893\n",
      "0.1419420567554227\n",
      "0.14132161259692894\n",
      "0.14070557281897078\n",
      "0.14009389876707093\n",
      "0.13948655214768624\n",
      "0.1388834950253423\n",
      "0.13828468981976816\n",
      "0.13769009930303353\n",
      "0.13709968659668717\n",
      "0.13651341516889953\n",
      "0.13593124883160812\n",
      "0.135353151737669\n",
      "0.13477908837801242\n",
      "0.1342090235788046\n",
      "0.13364292249861615\n",
      "0.13308075062559854\n",
      "0.13252247377466714\n",
      "0.1319680580846935\n",
      "0.13141747001570644\n",
      "0.1308706763461025\n",
      "0.13032764416986628\n",
      "0.1297883408938018\n",
      "0.12925273423477412\n",
      "0.12872079221696262\n",
      "0.12819248316912663\n",
      "0.12766777572188207\n",
      "0.1271466388049921\n",
      "0.12662904164466962\n",
      "0.12611495376089343\n",
      "0.1256043449647387\n",
      "0.12509718535572056\n",
      "0.12459344531915201\n",
      "0.12409309552351734\n",
      "0.12359610691785916\n",
      "0.12310245072918154\n",
      "0.12261209845986765\n",
      "0.12212502188511373\n",
      "0.12164119305037874\n",
      "0.12116058426884885\n",
      "0.12068316811892044\n",
      "0.12020891744169651\n",
      "0.1197378053385026\n",
      "0.11926980516841701\n",
      "0.11880489054581946\n",
      "0.1183430353379554\n",
      "0.11788421366251878\n",
      "0.11742839988525089\n",
      "0.11697556861755705\n",
      "0.11652569471414038\n",
      "0.11607875327065356\n",
      "0.11563471962136765\n",
      "0.11519356933685876\n",
      "0.11475527822171216\n",
      "0.11431982231224444\n",
      "0.11388717787424359\n",
      "0.11345732140072601\n",
      "0.11303022960971251\n",
      "0.11260587944202127\n",
      "0.11218424805907913\n",
      "0.11176531284075081\n",
      "0.11134905138318574\n",
      "0.11093544149668286\n",
      "0.1105244612035732\n",
      "0.11011608873612096\n",
      "0.10971030253444142\n",
      "0.10930708124443739\n",
      "0.10890640371575318\n",
      "0.10850824899974644\n",
      "0.1081125963474775\n",
      "0.1077194252077166\n",
      "0.10732871522496881\n",
      "0.10694044623751589\n",
      "0.10655459827547673\n",
      "0.10617115155888414\n",
      "0.10579008649578009\n",
      "w = [[0.80956046]\n",
      " [2.0508202 ]]\n",
      "b = 1.5948713189708588\n",
      "dw = [[ 0.17860505]\n",
      " [-0.04840656]]\n",
      "db = -0.08888460336847771\n",
      "Costs = [array(0.15900538)]\n",
      "5.801545319394553\n",
      "5.128997363129296\n",
      "4.461110016113006\n",
      "3.803854909341708\n",
      "3.1695765407009526\n",
      "2.5797670555692536\n",
      "2.0609711342727994\n",
      "1.6293316677137626\n",
      "1.2807693347266655\n",
      "1.0021405215914179\n",
      "0.7846865584622621\n",
      "0.6249129478069452\n",
      "0.5178556560528647\n",
      "0.45269830500748676\n",
      "0.4157012831623389\n",
      "0.39532860718676394\n",
      "0.3840794050572001\n",
      "0.3776899242618641\n",
      "0.37387332927337036\n",
      "0.3714250827511729\n",
      "0.3697107409596033\n",
      "0.3683938329498736\n",
      "0.3672943646822116\n",
      "0.36631524512666125\n",
      "0.36540372127745757\n",
      "0.36453095618161235\n",
      "0.3636811068068745\n",
      "0.36284543167332967\n",
      "0.3620190889009834\n",
      "0.36119938601843576\n",
      "0.3603848188229016\n",
      "0.35957454160139807\n",
      "0.3587680742635751\n",
      "0.3579651400678192\n",
      "0.3571655755427696\n",
      "0.35636928041163124\n",
      "0.35557618972188393\n",
      "0.35478625831895944\n",
      "0.3539994521898857\n",
      "0.35321574363391617\n",
      "0.3524351085666113\n",
      "0.35165752501394565\n",
      "0.35088297227049897\n",
      "0.3501114304283366\n",
      "0.34934288011284176\n",
      "0.3485773023340869\n",
      "0.34781467840269875\n",
      "0.3470549898817147\n",
      "0.34629821855851595\n",
      "0.34554434642795495\n",
      "0.3447933556817241\n",
      "0.3440452287012033\n",
      "0.3432999480522544\n",
      "0.34255749648110645\n",
      "0.34181785691086164\n",
      "0.3410810124383615\n",
      "0.34034694633126883\n",
      "0.3396156420252901\n",
      "0.33888708312149257\n",
      "0.33816125338369646\n",
      "0.33743813673592854\n",
      "0.3367177172599294\n",
      "0.3359999791927129\n",
      "0.3352849069241759\n",
      "0.33457248499475273\n",
      "0.3338626980931204\n",
      "0.3331555310539458\n",
      "0.3324509688556815\n",
      "0.33174899661840307\n",
      "0.33104959960169156\n",
      "0.3303527632025572\n",
      "0.3296584729534056\n",
      "0.3289667145200446\n",
      "0.32827747369972937\n",
      "0.3275907364192494\n",
      "0.3269064887330499\n",
      "0.3262247168213931\n",
      "0.32554540698855494\n",
      "0.324868545661056\n",
      "0.3241941193859294\n",
      "0.32352211482901994\n",
      "0.322852518773318\n",
      "0.3221853181173251\n",
      "0.3215204998734502\n",
      "0.3208580511664383\n",
      "0.32019795923182726\n",
      "0.31954021141443545\n",
      "0.31888479516687707\n",
      "0.318231698048106\n",
      "0.3175809077219864\n",
      "0.3169324119558907\n",
      "0.3162861986193234\n",
      "0.3156422556825698\n",
      "0.31500057121537106\n",
      "0.3143611333856222\n",
      "0.3137239304580944\n",
      "0.31308895079318066\n",
      "0.31245618284566534\n",
      "0.3118256151635126\n",
      "0.31119723638668084\n",
      "0.31057103524595553\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Exercise 7 - predict\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56419b97ebf382a8f93ac2873988887",
     "grade": false,
     "grade_id": "cell-d6f924f49c51dc2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    #(≈ 1 line of code)\n",
    "    # A = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z = np.dot(w.T, X) + b \n",
    "    A = sigmoid(Z)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        #(≈ 4 lines of code)\n",
    "        # if A[0, i] > ____ :\n",
    "        #     Y_prediction[0,i] = \n",
    "        # else:\n",
    "        #     Y_prediction[0,i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        if A[0, i] > 0.5 :\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3ea12608f15798d542a07c1bc9f561b",
     "grade": true,
     "grade_id": "cell-90b1fb967269548c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember:**\n",
    "    \n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - Computing the cost and its gradient \n",
    "    - Updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 - model\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - parameters, grads, costs for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b62adfb8f5a0f5bb5aa6798c3c5df66d",
     "grade": false,
     "grade_id": "cell-6dcba5967c4cbf8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)   \n",
    "    # initialize parameters with zeros\n",
    "    # and use the \"shape\" function to get the first dimension of X_train\n",
    "    # w, b = ...\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Gradient descent \n",
    "    # params, grads, costs = ...\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    # Y_prediction_test = ...\n",
    "    # Y_prediction_train = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    w,b=initialize_with_zeros(X_train.shape[0])\n",
    "    params, grads, costs =optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w=params[\"w\"]\n",
    "    b=params[\"b\"]\n",
    "                                                                   \n",
    "    Y_prediction_test=predict(w,b,X_test)\n",
    "    Y_prediction_train=predict(w,b,X_train)\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b468bc5ddf6ecc5c7dbcb9a02cfe0216",
     "grade": true,
     "grade_id": "cell-4170e070f3cde17e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.6909490977063016\n",
      "0.6887671562212799\n",
      "0.6866012114471467\n",
      "0.6844511196826286\n",
      "0.6823167381945483\n",
      "0.6801979252286898\n",
      "0.6780945400199205\n",
      "0.6760064428015814\n",
      "0.6739334948141704\n",
      "0.6718755583133357\n",
      "0.669832496577196\n",
      "0.6678041739130095\n",
      "0.6657904556632107\n",
      "0.6637912082108302\n",
      "0.6618062989843178\n",
      "0.6598355964617889\n",
      "0.6578789701747101\n",
      "0.6559362907110416\n",
      "0.6540074297178562\n",
      "0.6520922599034492\n",
      "0.6501906550389611\n",
      "0.6483024899595237\n",
      "0.6464276405649514\n",
      "0.6445659838199927\n",
      "0.6427173977541565\n",
      "0.6408817614611295\n",
      "0.6390589550978029\n",
      "0.6372488598829185\n",
      "0.6354513580953541\n",
      "0.6336663330720596\n",
      "0.6318936692056589\n",
      "0.6301332519417322\n",
      "0.6283849677757907\n",
      "0.6266487042499593\n",
      "0.624924349949378\n",
      "0.6232117944983354\n",
      "0.6215109285561479\n",
      "0.6198216438127949\n",
      "0.6181438329843209\n",
      "0.6164773898080201\n",
      "0.6148222090374097\n",
      "0.6131781864370045\n",
      "0.6115452187769049\n",
      "0.6099232038272054\n",
      "0.6083120403522356\n",
      "0.6067116281046432\n",
      "0.6051218678193263\n",
      "0.6035426612072263\n",
      "0.6019739109489892\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "from public_tests import *\n",
    "\n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass all the tests, run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "Cost after iteration 0: 0.693147\n",
      "0.7410294145065183\n",
      "0.7531535818862111\n",
      "0.8667086956701203\n",
      "0.7685635006608347\n",
      "0.8970144736447933\n",
      "0.7556132280568215\n",
      "0.880436642683987\n",
      "0.7514852682041282\n",
      "0.8771330533999597\n",
      "0.7449403578103343\n",
      "0.869479030571014\n",
      "0.7395380761496434\n",
      "0.8631813827864874\n",
      "0.7341140373099903\n",
      "0.8564988745006686\n",
      "0.7289883166665418\n",
      "0.8499947121719625\n",
      "0.7240300591410962\n",
      "0.843519150240876\n",
      "0.7192489303106331\n",
      "0.8371297668639592\n",
      "0.7146178461453454\n",
      "0.8308196474365936\n",
      "0.7101237125685045\n",
      "0.8245968156414776\n",
      "0.7057525912197258\n",
      "0.818462789308895\n",
      "0.7014932393013824\n",
      "0.8124189543586325\n",
      "0.6973357819554408\n",
      "0.8064653562757486\n",
      "0.6932717128504409\n",
      "0.8006013474032809\n",
      "0.6892936205327379\n",
      "0.7948256977270329\n",
      "0.6853950268137255\n",
      "0.7891367737081703\n",
      "0.6815702381349416\n",
      "0.7835326559229573\n",
      "0.677814225655342\n",
      "0.7780112344915432\n",
      "0.6741225251755734\n",
      "0.7725702816054895\n",
      "0.6704911541755105\n",
      "0.7672075072052831\n",
      "0.6669165427749882\n",
      "0.7619206012194385\n",
      "0.6633954761707312\n",
      "0.7567072652986898\n",
      "0.6599250465088032\n",
      "0.7515652363088827\n",
      "0.6565026125262358\n",
      "0.7464923033757681\n",
      "0.6531257655972149\n",
      "0.7414863198969834\n",
      "0.6497923010676048\n",
      "0.7365452116406302\n",
      "0.6465001939645267\n",
      "0.7316669818161801\n",
      "0.643247578333392\n",
      "0.7268497138186596\n",
      "0.6400327295898608\n",
      "0.7220915722005701\n",
      "0.6368540493842914\n",
      "0.7173908023096314\n",
      "0.6337100525659516\n",
      "0.7127457289378907\n",
      "0.630599355907401\n",
      "0.7081547542540253\n",
      "0.6275206683091039\n",
      "0.7036163552319201\n",
      "0.6244727822530355\n",
      "0.6991290807417755\n",
      "0.6214545663138489\n",
      "0.6946915484326741\n",
      "0.6184649585687424\n",
      "0.6903024415058099\n",
      "0.6155029607738625\n",
      "0.6859605054539332\n",
      "0.6125676331969916\n",
      "0.6816645448237989\n",
      "0.6096580900142922\n",
      "0.6774134200435374\n",
      "0.6067734951937223\n",
      "0.673206044345146\n",
      "0.6039130588000032\n",
      "0.669041380803082\n",
      "0.6010760336661553\n",
      "0.6649184395027571\n",
      "0.5982617123850421\n",
      "0.6608362748471476\n",
      "0.5954694245813507\n",
      "0.6567939830054662\n",
      "0.5926985344302728\n",
      "0.6527906995045833\n",
      "0.589948438394027\n",
      "0.6488255969614726\n",
      "0.587218563151448\n",
      "0.6448978829531701\n",
      "0.5845083636993086\n",
      "Cost after iteration 100: 0.584508\n",
      "0.6410067980194971\n",
      "0.5818173216069348\n",
      "0.637151613792928\n",
      "0.5791449434081318\n",
      "0.6333316312494625\n",
      "0.5764907591165144\n",
      "0.629546179074062\n",
      "0.5738543208521087\n",
      "0.6257946121341074\n",
      "0.5712352015686086\n",
      "0.6220763100543738\n",
      "0.5686329938719571\n",
      "0.6183906758871474\n",
      "0.5660473089220475\n",
      "0.6147371348713364\n",
      "0.5634777754102926\n",
      "0.6111151332746846\n",
      "0.5609240386066406\n",
      "0.6075241373134934\n",
      "0.5583857594703387\n",
      "0.6039636321445838\n",
      "0.5558626138193703\n",
      "0.6004331209245504\n",
      "0.553354291554036\n",
      "0.5969321239316909\n",
      "0.550860495930629\n",
      "0.5934601777463145\n",
      "0.5483809428815709\n",
      "0.5900168344854453\n",
      "0.545915360378744\n",
      "0.5866016610882367\n",
      "0.5434634878370806\n",
      "0.5832142386486987\n",
      "0.5410250755557533\n",
      "0.5798541617926044\n",
      "0.5385998841945687\n",
      "0.5765210380956987\n",
      "0.5361876842833909\n",
      "0.5732144875405625\n",
      "0.5337882557626193\n",
      "0.5699341420097105\n",
      "0.5314013875529303\n",
      "0.5666796448126956\n",
      "0.5290268771526502\n",
      "0.5634506502451883\n",
      "0.526664530261273\n",
      "0.5602468231781692\n",
      "0.5243141604277676\n",
      "0.5570678386755336\n",
      "0.521975588722439\n",
      "0.5539133816385552\n",
      "0.519648643431216\n",
      "0.5507831464757948\n",
      "0.5173331597713365\n",
      "0.547676836797164\n",
      "0.5150289796274968\n",
      "0.5445941651309734\n",
      "0.5127359513076122\n",
      "0.5415348526629059\n",
      "0.5104539293174177\n",
      "0.5384986289959504\n",
      "0.5081827741532154\n",
      "0.5354852319304387\n",
      "0.5059223521121377\n",
      "0.5324944072634062\n",
      "0.5036725351193724\n",
      "0.5295259086065934\n",
      "0.501433200571856\n",
      "0.5265794972224744\n",
      "0.49920423119800217\n",
      "0.5236549418777879\n",
      "0.4969855149330984\n",
      "0.5207520187141109\n",
      "0.4947769448100627\n",
      "0.5178705111350965\n",
      "0.4925784188653096\n",
      "0.5150102097100576\n",
      "0.49038984005953834\n",
      "0.5121709120936565\n",
      "0.48821111621331154\n",
      "0.5093524229615245\n",
      "0.48604215995736\n",
      "0.5065545539617037\n",
      "0.4838828886976017\n",
      "0.5037771236818703\n",
      "0.48173322459493667\n",
      "0.5010199576323695\n",
      "0.4795930945599332\n",
      "0.498282888245157\n",
      "0.4774624302625984\n",
      "0.49556575488881527\n",
      "0.47534116815748434\n",
      "0.49286840389987974\n",
      "0.473229249524463\n",
      "0.49019068863078447\n",
      "0.47112662052556814\n",
      "0.48753246951480933\n",
      "0.469033232278386\n",
      "0.4848936141484849\n",
      "0.46694904094655476\n",
      "Cost after iteration 200: 0.466949\n",
      "0.48227399739198706\n",
      "0.46487400784801647\n",
      "0.47967350148813304\n",
      "0.46280809958175345\n",
      "0.4770920162006677\n",
      "0.46075128817383065\n",
      "0.4745294389726099\n",
      "0.45870355124366385\n",
      "0.4719856751055088\n",
      "0.4566648721915241\n",
      "0.4694606379605424\n",
      "0.45463524040839887\n",
      "0.4669542491824678\n",
      "0.4526146515094255\n",
      "0.4644664389475178\n",
      "0.45060310759222383\n",
      "0.4619971462364038\n",
      "0.44860061752155944\n",
      "0.45954631913366956\n",
      "0.44660719724187353\n",
      "0.45711391515469313\n",
      "0.4446228701193258\n",
      "0.45469990160169954\n",
      "0.4426476673150916\n",
      "0.4523042559501854\n",
      "0.440681628191754\n",
      "0.44992696626718864\n",
      "0.4387248007547159\n",
      "0.4475680316628426\n",
      "0.4367772421306287\n",
      "0.4452274627766381\n",
      "0.434839019084887\n",
      "0.44290528229976717\n",
      "0.43291020858027174\n",
      "0.44060152553483356\n",
      "0.4309908983788174\n",
      "0.4383162409940765\n",
      "0.4290811876889355\n",
      "0.436049491037057\n",
      "0.42718118785973686\n",
      "0.43380135254848395\n",
      "0.4252910231243292\n",
      "0.43157191765650327\n",
      "0.42341083139363334\n",
      "0.4293612944913042\n",
      "0.42154076510192434\n",
      "0.4271696079833187\n",
      "0.41968099210485144\n",
      "0.42499700069954927\n",
      "0.41783169663009556\n",
      "0.42284363371566197\n",
      "0.4159930802800587\n",
      "0.42070968752036725\n",
      "0.4141653630850181\n",
      "0.41859536294728333\n",
      "0.4123487846039702\n",
      "0.416500882127863\n",
      "0.41054360506891785\n",
      "0.4144264894570632\n",
      "0.4087501065665542\n",
      "0.412372452561184\n",
      "0.40696859424913157\n",
      "0.41033906325467534\n",
      "0.4051993975637267\n",
      "0.40832663846965433\n",
      "0.403442871486063\n",
      "0.4063355211383761\n",
      "0.40169939774148966\n",
      "0.40436608100490584\n",
      "0.3999693859915878\n",
      "0.4024187153377506\n",
      "0.3982532749601495\n",
      "0.40049384951020334\n",
      "0.39655153346691663\n",
      "0.3985919374096682\n",
      "0.3948646613314855\n",
      "0.3967134616312998\n",
      "0.393193190103184\n",
      "0.3948589334050027\n",
      "0.3915376835655914\n",
      "0.39302889219833875\n",
      "0.3898987379567916\n",
      "0.3912239049313531\n",
      "0.3882769818386255\n",
      "0.3894445647330716\n",
      "0.38667307554038083\n",
      "0.38769148916375384\n",
      "0.3850877100948939\n",
      "0.38596531782241084\n",
      "0.3835216055784058\n",
      "0.3842667092561593\n",
      "0.38197550876031106\n",
      "0.3825963370873796\n",
      "0.3804501899658872\n",
      "0.38095488527715726\n",
      "0.3789464390550751\n",
      "0.37934304245001105\n",
      "0.37746506042437517\n",
      "0.37776149521638075\n",
      "0.3760068669480208\n",
      "Cost after iteration 300: 0.376007\n",
      "0.3762109204467301\n",
      "0.37457267278992734\n",
      "0.37469197647530555\n",
      "0.37316328504057084\n",
      "0.3732052932433021\n",
      "0.3717794941639073\n",
      "0.37175146143088006\n",
      "0.37042206327935184\n",
      "0.3703310206751294\n",
      "0.36909171635294324\n",
      "0.3689444470261166\n",
      "0.36778912542970976\n",
      "0.3675921398541885\n",
      "0.36651489710468893\n",
      "0.36627440848651155\n",
      "0.36526955850080733\n",
      "0.3649914589161128\n",
      "0.36405354309454374\n",
      "0.3637433809882394\n",
      "0.36286717680046654\n",
      "0.3625301365215135\n",
      "0.3617106647878135\n",
      "0.36135154885938003\n",
      "0.36058407954990906\n",
      "0.3602072943647401\n",
      "0.35948735077372707\n",
      "0.35909689636184383\n",
      "0.35842025755590257\n",
      "0.35801972198991616\n",
      "0.35738242347768184\n",
      "0.3569749823599136\n",
      "0.35637331498127545\n",
      "0.3559617362990389\n",
      "0.3553922433832199\n",
      "0.3549788978301163\n",
      "0.3544383707194073\n",
      "0.35402524737087143\n",
      "0.3535107194478467\n",
      "0.35309944646108943\n",
      "0.35260818584905296\n",
      "0.352200055645648\n",
      "0.3517295567731742\n",
      "0.3513255549722941\n",
      "0.3508735292024186\n",
      "0.35047436641871327\n",
      "0.3500387319421303\n",
      "0.34964487745657047\n",
      "0.3492237486376543\n",
      "0.34883546490049694\n",
      "0.3484271412474876\n",
      "0.3480445181830923\n",
      "0.34764747309218996\n",
      "0.3472704612436108\n",
      "0.346883330643885\n",
      "0.3465117723137941\n",
      "0.3461333433182681\n",
      "0.3457670010205013\n",
      "0.3453962006704071\n",
      "0.3450347823891382\n",
      "0.3446706665642172\n",
      "0.34431384751029614\n",
      "0.3439555900683577\n",
      "0.3436030308100657\n",
      "0.343249913013274\n",
      "0.34290127402909465\n",
      "0.3425526743115566\n",
      "0.3422076271560245\n",
      "0.3418630112858342\n",
      "0.34152124667020156\n",
      "0.3411801583577954\n",
      "0.3408413915228863\n",
      "0.34050344352519896\n",
      "0.3401674173254017\n",
      "0.33983228309107555\n",
      "0.33949876921950983\n",
      "0.3391661751139331\n",
      "0.338834973884711\n",
      "0.33850469202496786\n",
      "0.3381756310953305\n",
      "0.33784747281466143\n",
      "0.33752040518390736\n",
      "0.3371942151336296\n",
      "0.33686901670298997\n",
      "0.3365446675878064\n",
      "0.33622123451072916\n",
      "0.3358986224417361\n",
      "0.33557686844128914\n",
      "0.3352559088804744\n",
      "0.33493576266251984\n",
      "0.33461638692366746\n",
      "0.3342977897727745\n",
      "0.3339799420368132\n",
      "0.3336628456473471\n",
      "0.3333464804454546\n",
      "0.3330308450129771\n",
      "0.3327159251281503\n",
      "0.3324017177057243\n",
      "0.3320882124429035\n",
      "0.3317754055523588\n",
      "0.3314632893282512\n",
      "Cost after iteration 400: 0.331463\n",
      "0.3311518598069663\n",
      "0.3308411110131456\n",
      "0.3305310390714592\n",
      "0.3302216391677471\n",
      "0.32991290762976655\n",
      "0.3296048404290039\n",
      "0.32929743412947643\n",
      "0.3289906852392287\n",
      "0.32868459055055926\n",
      "0.32837914694183484\n",
      "0.3280743514076863\n",
      "0.3277702010851469\n",
      "0.32746669313988536\n",
      "0.32716382489215456\n",
      "0.32686159364838524\n",
      "0.32655999686081055\n",
      "0.32625903195014816\n",
      "0.3259586964656832\n",
      "0.3256589879205829\n",
      "0.3253599039373208\n",
      "0.3250614421041722\n",
      "0.3247636001004829\n",
      "0.3244663755762174\n",
      "0.3241697662564421\n",
      "0.3238737698426167\n",
      "0.3235783840979146\n",
      "0.32328360676763923\n",
      "0.32298943564787824\n",
      "0.32269586852207993\n",
      "0.3224029032157023\n",
      "0.3221105375461066\n",
      "0.3218187693656942\n",
      "0.32152759652259105\n",
      "0.32123701689446565\n",
      "0.3209470283578526\n",
      "0.32065762881450965\n",
      "0.32036881616759727\n",
      "0.32008058834211284\n",
      "0.3197929432664718\n",
      "0.3195058788882727\n",
      "0.3192193931601114\n",
      "0.3189334840516853\n",
      "0.3186481495389063\n",
      "0.318363387613153\n",
      "0.31807919627294196\n",
      "0.31779557353096666\n",
      "0.31751251740774666\n",
      "0.31723002593695554\n",
      "0.31694809716059336\n",
      "0.316666729132999\n",
      "0.3163859199171893\n",
      "0.3161056675878645\n",
      "0.3158259702286378\n",
      "0.31554682593427474\n",
      "0.31526823280859867\n",
      "0.3149901889661485\n",
      "0.3147126925305975\n",
      "0.3144357416359706\n",
      "0.31415933442545113\n",
      "0.3138834690522661\n",
      "0.3136081436787861\n",
      "0.31333335647716076\n",
      "0.3130591056286382\n",
      "0.31278538932401545\n",
      "0.3125122057631206\n",
      "0.3122395531551266\n",
      "0.31196742971815455\n",
      "0.3116958336794862\n",
      "0.3114247632752577\n",
      "0.3111542167505977\n",
      "0.3108841923593873\n",
      "0.31061468836434425\n",
      "0.31034570303683257\n",
      "0.31007723465690645\n",
      "0.3098092815131568\n",
      "0.30954184190272704\n",
      "0.30927491413118563\n",
      "0.309008496512522\n",
      "0.3087425873690391\n",
      "0.30847718503133414\n",
      "0.308212287838207\n",
      "0.3079478941366298\n",
      "0.30768400228166615\n",
      "0.30742061063643483\n",
      "0.3071577175720361\n",
      "0.306895321467511\n",
      "0.3066334207097741\n",
      "0.3063720136935693\n",
      "0.30611109882140763\n",
      "0.3058506745035208\n",
      "0.30559073915780266\n",
      "0.30533129120976177\n",
      "0.30507232909246595\n",
      "0.304813851246494\n",
      "0.3045558561198827\n",
      "0.3042983421680781\n",
      "0.30404130785388517\n",
      "0.3037847516474191\n",
      "0.3035286720260554\n",
      "0.3032730674743829\n",
      "Cost after iteration 500: 0.303273\n",
      "0.30301793648415537\n",
      "0.3027632775542439\n",
      "0.30250908919059066\n",
      "0.3022553699061614\n",
      "0.3020021182209005\n",
      "0.3017493326616845\n",
      "0.3014970117622774\n",
      "0.30124515406328556\n",
      "0.30099375811211365\n",
      "0.3007428224629202\n",
      "0.3004923456765745\n",
      "0.30024232632061343\n",
      "0.2999927629691978\n",
      "0.2997436542030715\n",
      "0.2994949986095177\n",
      "0.29924679478231886\n",
      "0.2989990413217143\n",
      "0.2987517368343596\n",
      "0.2985048799332863\n",
      "0.29825846923786115\n",
      "0.2980125033737469\n",
      "0.29776698097286247\n",
      "0.2975219006733435\n",
      "0.29727726111950403\n",
      "0.29703306096179777\n",
      "0.29678929885677985\n",
      "0.296545973467069\n",
      "0.2963030834613101\n",
      "0.29606062751413714\n",
      "0.29581860430613566\n",
      "0.2955770125238072\n",
      "0.29533585085953207\n",
      "0.2950951180115338\n",
      "0.29485481268384356\n",
      "0.29461493358626434\n",
      "0.2943754794343363\n",
      "0.2941364489493019\n",
      "0.2938978408580711\n",
      "0.2936596538931872\n",
      "0.29342188679279324\n",
      "0.29318453830059793\n",
      "0.29294760716584234\n",
      "0.2927110921432665\n",
      "0.2924749919930772\n",
      "0.29223930548091465\n",
      "0.29200403137782055\n",
      "0.29176916846020573\n",
      "0.2915347155098185\n",
      "0.2913006713137132\n",
      "0.2910670346642181\n",
      "0.2908338043589054\n",
      "0.2906009792005594\n",
      "0.2903685579971465\n",
      "0.29013653956178465\n",
      "0.28990492271271295\n",
      "0.28967370627326217\n",
      "0.2894428890718248\n",
      "0.2892124699418256\n",
      "0.28898244772169246\n",
      "0.28875282125482754\n",
      "0.2885235893895781\n",
      "0.28829475097920837\n",
      "0.28806630488187074\n",
      "0.2878382499605782\n",
      "0.2876105850831761\n",
      "0.28738330912231425\n",
      "0.2871564209554198\n",
      "0.2869299194646698\n",
      "0.286703803536964\n",
      "0.28647807206389825\n",
      "0.2862527239417374\n",
      "0.28602775807138925\n",
      "0.28580317335837785\n",
      "0.2855789687128181\n",
      "0.28535514304938864\n",
      "0.28513169528730753\n",
      "0.28490862435030545\n",
      "0.2846859291666015\n",
      "0.28446360866887727\n",
      "0.2842416617942518\n",
      "0.28402008748425767\n",
      "0.28379888468481546\n",
      "0.2835780523462097\n",
      "0.2833575894230648\n",
      "0.2831374948743205\n",
      "0.2829177676632083\n",
      "0.2826984067572278\n",
      "0.2824794111281227\n",
      "0.2822607797518581\n",
      "0.2820425116085962\n",
      "0.2818246056826746\n",
      "0.28160706096258215\n",
      "0.281389876440937\n",
      "0.28117305111446383\n",
      "0.280956583983971\n",
      "0.2807404740543291\n",
      "0.28052472033444786\n",
      "0.2803093218372554\n",
      "0.28009427757967503\n",
      "0.2798795865826048\n",
      "Cost after iteration 600: 0.279880\n",
      "0.2796652478708956\n",
      "0.2794512604733291\n",
      "0.27923762342259806\n",
      "0.2790243357552837\n",
      "0.278811396511836\n",
      "0.27859880473655224\n",
      "0.2783865594775564\n",
      "0.27817465978677935\n",
      "0.27796310471993746\n",
      "0.27775189333651334\n",
      "0.27754102469973524\n",
      "0.27733049787655767\n",
      "0.2771203119376406\n",
      "0.2769104659573308\n",
      "0.276700959013642\n",
      "0.27649179018823555\n",
      "0.27628295856640067\n",
      "0.27607446323703605\n",
      "0.2758663032926304\n",
      "0.27565847782924385\n",
      "0.2754509859464891\n",
      "0.27524382674751224\n",
      "0.2750369993389754\n",
      "0.27483050283103755\n",
      "0.27462433633733624\n",
      "0.27441849897496984\n",
      "0.27421298986447956\n",
      "0.2740078081298311\n",
      "0.27380295289839734\n",
      "0.27359842330094025\n",
      "0.273394218471594\n",
      "0.27319033754784666\n",
      "0.2729867796705239\n",
      "0.27278354398377075\n",
      "0.2725806296350355\n",
      "0.27237803577505204\n",
      "0.27217576155782314\n",
      "0.27197380614060407\n",
      "0.27177216868388543\n",
      "0.2715708483513771\n",
      "0.27136984430999145\n",
      "0.2711691557298269\n",
      "0.27096878178415273\n",
      "0.27076872164939125\n",
      "0.2705689745051035\n",
      "0.2703695395339722\n",
      "0.2701704159217866\n",
      "0.2699716028574262\n",
      "0.26977309953284584\n",
      "0.2695749051430597\n",
      "0.269377018886126\n",
      "0.26917943996313176\n",
      "0.2689821675781775\n",
      "0.2687852009383625\n",
      "0.26858853925376935\n",
      "0.2683921817374489\n",
      "0.2681961276054062\n",
      "0.268000376076585\n",
      "0.26780492637285347\n",
      "0.2676097777189896\n",
      "0.26741492934266664\n",
      "0.26722038047443913\n",
      "0.26702613034772793\n",
      "0.2668321781988068\n",
      "0.26663852326678783\n",
      "0.2664451647936073\n",
      "0.2662521020240125\n",
      "0.266059334205547\n",
      "0.26586686058853753\n",
      "0.26567468042608006\n",
      "0.2654827929740262\n",
      "0.2652911974909699\n",
      "0.26509989323823385\n",
      "0.26490887947985603\n",
      "0.26471815548257716\n",
      "0.26452772051582635\n",
      "0.26433757385170936\n",
      "0.26414771476499443\n",
      "0.2639581425331001\n",
      "0.2637688564360823\n",
      "0.26357985575662096\n",
      "0.26339113978000817\n",
      "0.26320270779413507\n",
      "0.26301455908947924\n",
      "0.2628266929590927\n",
      "0.2626391086985891\n",
      "0.2624518056061316\n",
      "0.2622647829824207\n",
      "0.2620780401306817\n",
      "0.26189157635665317\n",
      "0.2617053909685747\n",
      "0.2615194832771747\n",
      "0.26133385259565894\n",
      "0.2611484982396986\n",
      "0.2609634195274182\n",
      "0.2607786157793845\n",
      "0.2605940863185948\n",
      "0.2604098304704651\n",
      "0.26022584756281886\n",
      "0.2600421369258757\n",
      "Cost after iteration 700: 0.260042\n",
      "0.25985869789223975\n",
      "0.25967552979688896\n",
      "0.2594926319771634\n",
      "0.2593100037727544\n",
      "0.25912764452569337\n",
      "0.258945553580341\n",
      "0.2587637302833763\n",
      "0.25858217398378547\n",
      "0.25840088403285144\n",
      "0.2582198597841432\n",
      "0.25803910059350454\n",
      "0.25785860581904424\n",
      "0.257678374821125\n",
      "0.25749840696235327\n",
      "0.2573187016075685\n",
      "0.25713925812383304\n",
      "0.25696007588042175\n",
      "0.2567811542488118\n",
      "0.2566024926026727\n",
      "0.2564240903178556\n",
      "0.25624594677238377\n",
      "0.2560680613464425\n",
      "0.2558904334223689\n",
      "0.25571306238464225\n",
      "0.25553594761987425\n",
      "0.25535908851679895\n",
      "0.2551824844662633\n",
      "0.25500613486121737\n",
      "0.2548300390967046\n",
      "0.2546541965698527\n",
      "0.25447860667986366\n",
      "0.2543032688280046\n",
      "0.2541281824175981\n",
      "0.25395334685401344\n",
      "0.2537787615446565\n",
      "0.2536044258989613\n",
      "0.25343033932838044\n",
      "0.2532565012463759\n",
      "0.25308291106841024\n",
      "0.2529095682119372\n",
      "0.2527364720963935\n",
      "0.2525636221431886\n",
      "0.2523910177756974\n",
      "0.2522186584192501\n",
      "0.2520465435011244\n",
      "0.25187467245053574\n",
      "0.25170304469862986\n",
      "0.25153165967847335\n",
      "0.251360516825045\n",
      "0.251189615575228\n",
      "0.25101895536780094\n",
      "0.2508485356434289\n",
      "0.25067835584465636\n",
      "0.25050841541589774\n",
      "0.25033871380342937\n",
      "0.25016925045538174\n",
      "0.2500000248217306\n",
      "0.24983103635428927\n",
      "0.24966228450670025\n",
      "0.24949376873442752\n",
      "0.249325488494748\n",
      "0.24915744324674413\n",
      "0.24898963245129555\n",
      "0.24882205557107115\n",
      "0.24865471207052173\n",
      "0.24848760141587153\n",
      "0.24832072307511135\n",
      "0.24815407651798965\n",
      "0.2479876612160058\n",
      "0.24782147664240228\n",
      "0.2476555222721567\n",
      "0.24748979758197473\n",
      "0.24732430205028227\n",
      "0.24715903515721832\n",
      "0.24699399638462688\n",
      "0.2468291852160503\n",
      "0.24666460113672176\n",
      "0.2465002436335574\n",
      "0.24633611219514998\n",
      "0.24617220631176082\n",
      "0.24600852547531307\n",
      "0.24584506917938437\n",
      "0.24568183691919984\n",
      "0.24551882819162502\n",
      "0.24535604249515877\n",
      "0.2451934793299261\n",
      "0.24503113819767164\n",
      "0.2448690186017522\n",
      "0.24470712004713036\n",
      "0.24454544204036727\n",
      "0.2443839840896161\n",
      "0.24422274570461497\n",
      "0.2440617263966804\n",
      "0.24390092567870053\n",
      "0.24374034306512868\n",
      "0.2435799780719764\n",
      "0.24341983021680674\n",
      "0.24325989901872846\n",
      "0.24310018399838845\n",
      "0.24294068467796615\n",
      "Cost after iteration 800: 0.242941\n",
      "0.24278140058116648\n",
      "0.24262233123321353\n",
      "0.24246347616084452\n",
      "0.24230483489230303\n",
      "0.2421464069573328\n",
      "0.24198819188717147\n",
      "0.24183018921454444\n",
      "0.24167239847365826\n",
      "0.2415148192001949\n",
      "0.24135745093130523\n",
      "0.24120029320560307\n",
      "0.24104334556315898\n",
      "0.24088660754549443\n",
      "0.24073007869557536\n",
      "0.24057375855780644\n",
      "0.24041764667802526\n",
      "0.24026174260349606\n",
      "0.24010604588290377\n",
      "0.23995055606634863\n",
      "0.23979527270533957\n",
      "0.23964019535278933\n",
      "0.23948532356300792\n",
      "0.23933065689169689\n",
      "0.23917619489594427\n",
      "0.23902193713421793\n",
      "0.23886788316636082\n",
      "0.23871403255358453\n",
      "0.2385603848584642\n",
      "0.23840693964493279\n",
      "0.23825369647827555\n",
      "0.2381006549251242\n",
      "0.23794781455345196\n",
      "0.23779517493256758\n",
      "0.23764273563311014\n",
      "0.23749049622704355\n",
      "0.23733845628765138\n",
      "0.23718661538953112\n",
      "0.23703497310858926\n",
      "0.2368835290220354\n",
      "0.23673228270837782\n",
      "0.23658123374741719\n",
      "0.23643038172024222\n",
      "0.2362797262092241\n",
      "0.2361292667980113\n",
      "0.2359790030715244\n",
      "0.23582893461595117\n",
      "0.23567906101874123\n",
      "0.23552938186860106\n",
      "0.235379896755489\n",
      "0.23523060527061027\n",
      "0.2350815070064118\n",
      "0.23493260155657728\n",
      "0.2347838885160224\n",
      "0.23463536748089\n",
      "0.23448703804854462\n",
      "0.2343388998175681\n",
      "0.23419095238775478\n",
      "0.2340431953601064\n",
      "0.2338956283368274\n",
      "0.23374825092132026\n",
      "0.2336010627181804\n",
      "0.23345406333319202\n",
      "0.23330725237332273\n",
      "0.23316062944671948\n",
      "0.23301419416270333\n",
      "0.23286794613176534\n",
      "0.23272188496556157\n",
      "0.23257601027690866\n",
      "0.23243032167977928\n",
      "0.23228481878929752\n",
      "0.2321395012217341\n",
      "0.23199436859450256\n",
      "0.23184942052615423\n",
      "0.23170465663637363\n",
      "0.23156007654597452\n",
      "0.23141567987689526\n",
      "0.2312714662521944\n",
      "0.23112743529604635\n",
      "0.2309835866337368\n",
      "0.23083991989165875\n",
      "0.23069643469730816\n",
      "0.23055313067927924\n",
      "0.23041000746726076\n",
      "0.23026706469203131\n",
      "0.2301243019854554\n",
      "0.22998171898047914\n",
      "0.2298393153111259\n",
      "0.2296970906124924\n",
      "0.2295550445207446\n",
      "0.22941317667311326\n",
      "0.22927148670788983\n",
      "0.22912997426442286\n",
      "0.2289886389831135\n",
      "0.22884748050541148\n",
      "0.2287064984738112\n",
      "0.22856569253184764\n",
      "0.22842506232409254\n",
      "0.2282846074961503\n",
      "0.22814432769465368\n",
      "0.22800422256726066\n",
      "Cost after iteration 900: 0.228004\n",
      "0.2278642917626498\n",
      "0.22772453493051675\n",
      "0.22758495172157014\n",
      "0.22744554178752788\n",
      "0.22730630478111347\n",
      "0.22716724035605138\n",
      "0.2270283481670644\n",
      "0.2268896278698691\n",
      "0.22675107912117223\n",
      "0.22661270157866706\n",
      "0.22647449490102936\n",
      "0.22633645874791417\n",
      "0.2261985927799516\n",
      "0.22606089665874346\n",
      "0.2259233700468596\n",
      "0.2257860126078341\n",
      "0.22564882400616162\n",
      "0.22551180390729408\n",
      "0.22537495197763666\n",
      "0.22523826788454462\n",
      "0.22510175129631937\n",
      "0.22496540188220518\n",
      "0.2248292193123857\n",
      "0.22469320325797995\n",
      "0.22455735339103958\n",
      "0.2244216693845448\n",
      "0.2242861509124011\n",
      "0.22415079764943596\n",
      "0.22401560927139513\n",
      "0.2238805854549394\n",
      "0.22374572587764122\n",
      "0.22361103021798115\n",
      "0.2234764981553446\n",
      "0.2233421293700186\n",
      "0.22320792354318833\n",
      "0.22307388035693354\n",
      "0.22293999949422566\n",
      "0.2228062806389245\n",
      "0.22267272347577463\n",
      "0.22253932769040224\n",
      "0.22240609296931235\n",
      "0.22227301899988472\n",
      "0.22214010547037125\n",
      "0.22200735206989283\n",
      "0.22187475848843569\n",
      "0.22174232441684863\n",
      "0.22161004954683974\n",
      "0.22147793357097303\n",
      "0.22134597618266585\n",
      "0.22121417707618507\n",
      "0.22108253594664437\n",
      "0.22095105249000146\n",
      "0.2208197264030542\n",
      "0.22068855738343834\n",
      "0.22055754512962394\n",
      "0.22042668934091267\n",
      "0.22029598971743447\n",
      "0.22016544596014484\n",
      "0.22003505777082175\n",
      "0.21990482485206256\n",
      "0.21977474690728122\n",
      "0.21964482364070528\n",
      "0.21951505475737262\n",
      "0.2193854399631292\n",
      "0.2192559789646254\n",
      "0.21912667146931386\n",
      "0.21899751718544577\n",
      "0.2188685158220689\n",
      "0.21873966708902398\n",
      "0.21861097069694213\n",
      "0.21848242635724222\n",
      "0.21835403378212773\n",
      "0.2182257926845844\n",
      "0.21809770277837653\n",
      "0.21796976377804517\n",
      "0.21784197539890515\n",
      "0.2177143373570416\n",
      "0.21758684936930814\n",
      "0.21745951115332357\n",
      "0.2173323224274693\n",
      "0.2172052829108868\n",
      "0.21707839232347456\n",
      "0.21695165038588565\n",
      "0.21682505681952505\n",
      "0.2166986113465467\n",
      "0.21657231368985141\n",
      "0.21644616357308363\n",
      "0.2163201607206291\n",
      "0.21619430485761226\n",
      "0.2160685957098935\n",
      "0.21594303300406675\n",
      "0.21581761646745667\n",
      "0.2156923458281162\n",
      "0.21556722081482416\n",
      "0.21544224115708244\n",
      "0.2153174065851135\n",
      "0.21519271682985788\n",
      "0.21506817162297198\n",
      "0.21494377069682472\n",
      "0.2148195137844964\n",
      "Cost after iteration 1000: 0.214820\n",
      "0.2146954006197746\n",
      "0.21457143093715314\n",
      "0.2144476044718286\n",
      "0.21432392095969838\n",
      "0.21420038013735837\n",
      "0.21407698174210002\n",
      "0.21395372551190836\n",
      "0.21383061118545937\n",
      "0.21370763850211755\n",
      "0.21358480720193376\n",
      "0.2134621170256427\n",
      "0.21333956771466026\n",
      "0.2132171590110819\n",
      "0.21309489065767956\n",
      "0.21297276239789958\n",
      "0.21285077397586052\n",
      "0.21272892513635067\n",
      "0.2126072156248257\n",
      "0.21248564518740654\n",
      "0.2123642135708771\n",
      "0.21224292052268176\n",
      "0.2121217657909233\n",
      "0.21200074912436043\n",
      "0.2118798702724058\n",
      "0.21175912898512364\n",
      "0.2116385250132276\n",
      "0.21151805810807833\n",
      "0.21139772802168147\n",
      "0.2112775345066853\n",
      "0.21115747731637885\n",
      "0.2110375562046891\n",
      "0.21091777092617958\n",
      "0.21079812123604755\n",
      "0.21067860689012224\n",
      "0.21055922764486257\n",
      "0.21043998325735497\n",
      "0.21032087348531134\n",
      "0.21020189808706688\n",
      "0.21008305682157785\n",
      "0.20996434944841982\n",
      "0.20984577572778526\n",
      "0.2097273354204814\n",
      "0.2096090282879287\n",
      "0.2094908540921579\n",
      "0.20937281259580884\n",
      "0.20925490356212773\n",
      "0.20913712675496565\n",
      "0.20901948193877617\n",
      "0.2089019688786132\n",
      "0.20878458734012956\n",
      "0.20866733708957436\n",
      "0.20855021789379136\n",
      "0.20843322952021684\n",
      "0.2083163717368775\n",
      "0.20819964431238877\n",
      "0.2080830470159527\n",
      "0.20796657961735598\n",
      "0.2078502418869679\n",
      "0.2077340335957388\n",
      "0.20761795451519757\n",
      "0.20750200441744998\n",
      "0.20738618307517712\n",
      "0.2072704902616327\n",
      "0.20715492575064196\n",
      "0.20703948931659938\n",
      "0.2069241807344667\n",
      "0.2068089997797712\n",
      "0.20669394622860385\n",
      "0.2065790198576174\n",
      "0.20646422044402435\n",
      "0.2063495477655956\n",
      "0.2062350016006581\n",
      "0.2061205817280931\n",
      "0.20600628792733447\n",
      "0.20589211997836707\n",
      "0.2057780776617243\n",
      "0.2056641607584871\n",
      "0.20555036905028132\n",
      "0.2054367023192768\n",
      "0.20532316034818499\n",
      "0.20520974292025707\n",
      "0.20509644981928277\n",
      "0.2049832808295882\n",
      "0.20487023573603427\n",
      "0.2047573143240146\n",
      "0.20464451637945413\n",
      "0.20453184168880761\n",
      "0.20441929003905715\n",
      "0.20430686121771105\n",
      "0.204194555012802\n",
      "0.2040823712128855\n",
      "0.20397030960703738\n",
      "0.2038583699848535\n",
      "0.20374655213644668\n",
      "0.20363485585244612\n",
      "0.20352328092399471\n",
      "0.20341182714274839\n",
      "0.20330049430087355\n",
      "0.2031892821910463\n",
      "0.20307819060644985\n",
      "Cost after iteration 1100: 0.203078\n",
      "0.2029672193407739\n",
      "0.20285636818821196\n",
      "0.2027456369434608\n",
      "0.20263502540171763\n",
      "0.20252453335867987\n",
      "0.20241416061054227\n",
      "0.20230390695399603\n",
      "0.2021937721862272\n",
      "0.2020837561049146\n",
      "0.20197385850822885\n",
      "0.20186407919483054\n",
      "0.2017544179638683\n",
      "0.20164487461497793\n",
      "0.20153544894828052\n",
      "0.20142614076438056\n",
      "0.20131694986436494\n",
      "0.2012078760498012\n",
      "0.2010989191227359\n",
      "0.20099007888569326\n",
      "0.20088135514167346\n",
      "0.2007727476941514\n",
      "0.20066425634707488\n",
      "0.20055588090486343\n",
      "0.20044762117240641\n",
      "0.20033947695506193\n",
      "0.20023144805865503\n",
      "0.20012353428947646\n",
      "0.2000157354542811\n",
      "0.19990805136028666\n",
      "0.19980048181517165\n",
      "0.19969302662707467\n",
      "0.19958568560459256\n",
      "0.199478458556779\n",
      "0.19937134529314315\n",
      "0.19926434562364803\n",
      "0.19915745935870946\n",
      "0.1990506863091942\n",
      "0.19894402628641888\n",
      "0.1988374791021484\n",
      "0.19873104456859453\n",
      "0.19862472249841487\n",
      "0.19851851270471058\n",
      "0.1984124150010263\n",
      "0.19830642920134728\n",
      "0.19820055512009938\n",
      "0.1980947925721468\n",
      "0.19798914137279086\n",
      "0.19788360133776917\n",
      "0.19777817228325337\n",
      "0.1976728540258487\n",
      "0.19756764638259217\n",
      "0.19746254917095088\n",
      "0.19735756220882167\n",
      "0.1972526853145289\n",
      "0.19714791830682366\n",
      "0.1970432610048819\n",
      "0.19693871322830345\n",
      "0.19683427479711121\n",
      "0.19672994553174888\n",
      "0.19662572525308014\n",
      "0.19652161378238753\n",
      "0.19641761094137092\n",
      "0.19631371655214616\n",
      "0.1962099304372441\n",
      "0.19610625241960883\n",
      "0.19600268232259702\n",
      "0.19589921996997625\n",
      "0.1957958651859237\n",
      "0.19569261779502525\n",
      "0.19558947762227386\n",
      "0.1954864444930685\n",
      "0.19538351823321298\n",
      "0.19528069866891457\n",
      "0.19517798562678274\n",
      "0.19507537893382823\n",
      "0.19497287841746133\n",
      "0.19487048390549114\n",
      "0.19476819522612404\n",
      "0.19466601220796276\n",
      "0.1945639346800049\n",
      "0.19446196247164166\n",
      "0.19436009541265725\n",
      "0.19425833333322698\n",
      "0.1941566760639165\n",
      "0.19405512343568038\n",
      "0.19395367527986093\n",
      "0.19385233142818764\n",
      "0.193751091712775\n",
      "0.19364995596612206\n",
      "0.1935489240211111\n",
      "0.19344799571100613\n",
      "0.1933471708694525\n",
      "0.19324644933047494\n",
      "0.19314583092847704\n",
      "0.19304531549823944\n",
      "0.1929449028749196\n",
      "0.19284459289404962\n",
      "0.19274438539153632\n",
      "0.1926442802036589\n",
      "0.19254427716706862\n",
      "Cost after iteration 1200: 0.192544\n",
      "0.19244437611878737\n",
      "0.19234457689620663\n",
      "0.19224487933708664\n",
      "0.19214528327955466\n",
      "0.19204578856210444\n",
      "0.19194639502359487\n",
      "0.19184710250324905\n",
      "0.19174791084065299\n",
      "0.19164881987575477\n",
      "0.1913521495721337\n",
      "0.19125345980470823\n",
      "0.19115486994011238\n",
      "0.19105637982044324\n",
      "0.19095798928815286\n",
      "0.1908596981860462\n",
      "0.1907615063572808\n",
      "0.19066341364536576\n",
      "0.1905654198941602\n",
      "0.19046752494787272\n",
      "0.19036972865106017\n",
      "0.19027203084862646\n",
      "0.19017443138582205\n",
      "0.19007693010824214\n",
      "0.18997952686182637\n",
      "0.1898822214928576\n",
      "0.18978501384796068\n",
      "0.1896879037741017\n",
      "0.1895908911185868\n",
      "0.1894939757290614\n",
      "0.18939715745350902\n",
      "0.18930043614025022\n",
      "0.18920381163794192\n",
      "0.18910728379557631\n",
      "0.18901085246247953\n",
      "0.18891451748831115\n",
      "0.18881827872306295\n",
      "0.18872213601705795\n",
      "0.18862608922094962\n",
      "0.18853013818572095\n",
      "0.18843428276268293\n",
      "0.1883385228034744\n",
      "0.18824285816006045\n",
      "0.18814728868473196\n",
      "0.18805181423010414\n",
      "0.18795643464911632\n",
      "0.1878611497950302\n",
      "0.18776595952142933\n",
      "0.18767086368221822\n",
      "0.18757586213162136\n",
      "0.18748095472418222\n",
      "0.18738614131476242\n",
      "0.18729142175854058\n",
      "0.18719679591101188\n",
      "0.18710226362798663\n",
      "0.18700782476558964\n",
      "0.1869134791802594\n",
      "0.18681922672874696\n",
      "0.18672506726811502\n",
      "0.18663100065573732\n",
      "0.1865370267492975\n",
      "0.18644314540678805\n",
      "0.18634935648651002\n",
      "0.18625565984707151\n",
      "0.18616205534738736\n",
      "0.18606854284667726\n",
      "0.1859751222044665\n",
      "0.1858817932805835\n",
      "0.18578855593515986\n",
      "0.18569541002862938\n",
      "0.18560235542172673\n",
      "0.18550939197548738\n",
      "0.18541651955124594\n",
      "0.1853237380106358\n",
      "0.1852310472155882\n",
      "0.1851384470283314\n",
      "0.18504593731138952\n",
      "0.18495351792758224\n",
      "0.18486118874002352\n",
      "0.18476894961212093\n",
      "0.18467680040757498\n",
      "0.1845847409903779\n",
      "0.1844927712248131\n",
      "0.18440089097545448\n",
      "0.18430910010716517\n",
      "0.18421739848509713\n",
      "0.18412578597469018\n",
      "0.18403426244167093\n",
      "0.18394282775205245\n",
      "0.18385148177213334\n",
      "0.18376022436849673\n",
      "0.18366905540800926\n",
      "0.18357797475782114\n",
      "0.18348698228536436\n",
      "0.18339607785835266\n",
      "0.1833052613447802\n",
      "0.18321453261292123\n",
      "0.18312389153132902\n",
      "0.18303333796883509\n",
      "Cost after iteration 1300: 0.183033\n",
      "0.18294287179454843\n",
      "0.18285249287785502\n",
      "0.18276220108841654\n",
      "0.18267199629617015\n",
      "0.1825818783713272\n",
      "0.18249184718437295\n",
      "0.18240190260606548\n",
      "0.18231204450743496\n",
      "0.18222227275978312\n",
      "0.18213258723468223\n",
      "0.18204298780397454\n",
      "0.1819534743397714\n",
      "0.18186404671445255\n",
      "0.18177470480066546\n",
      "0.1816854484713245\n",
      "0.18159627759961017\n",
      "0.18150719205896854\n",
      "0.1814181917231102\n",
      "0.18132927646600996\n",
      "0.18124044616190574\n",
      "0.1811517006852981\n",
      "0.1810630399109492\n",
      "0.18097446371388243\n",
      "0.18088597196938172\n",
      "0.18079756455299031\n",
      "0.18070924134051058\n",
      "0.1806210022080032\n",
      "0.18053284703178626\n",
      "0.18044477568843456\n",
      "0.18035678805477925\n",
      "0.1802688840079067\n",
      "0.18018106342515805\n",
      "0.18009332618412865\n",
      "0.18000567216266666\n",
      "0.17991810123887353\n",
      "0.17983061329110214\n",
      "0.17974320819795675\n",
      "0.1796558858382924\n",
      "0.17956864609121376\n",
      "0.1794814888360748\n",
      "0.17939441395247802\n",
      "0.17930742132027372\n",
      "0.17922051081955948\n",
      "0.17913368233067933\n",
      "0.17904693573422314\n",
      "0.17896027091102595\n",
      "0.17887368774216733\n",
      "0.1787871861089705\n",
      "0.17870076589300235\n",
      "0.17861442697607172\n",
      "0.17852816924022968\n",
      "0.17844199256776852\n",
      "0.17835589684122088\n",
      "0.1782698819433594\n",
      "0.1781839477571962\n",
      "0.17809809416598182\n",
      "0.17801232105320478\n",
      "0.17792662830259096\n",
      "0.17784101579810307\n",
      "0.1777554834239398\n",
      "0.17767003106453527\n",
      "0.17758465860455844\n",
      "0.1774993659289124\n",
      "0.1774141529227338\n",
      "0.1773290194713923\n",
      "0.17724396546048962\n",
      "0.17715899077585967\n",
      "0.17707409530356671\n",
      "0.17698927892990593\n",
      "0.17690454154140234\n",
      "0.17681988302481\n",
      "0.1767353032671115\n",
      "0.1766508021555177\n",
      "0.17656637957746668\n",
      "0.1764820354206233\n",
      "0.1763977695728785\n",
      "0.17631358192234908\n",
      "0.1762294723573767\n",
      "0.17614544076652722\n",
      "0.17606148703859048\n",
      "0.17597761106257942\n",
      "0.17589381272772958\n",
      "0.1758100919234988\n",
      "0.17572644853956587\n",
      "0.17564288246583068\n",
      "0.17555939359241346\n",
      "0.17547598180965404\n",
      "0.17539264700811108\n",
      "0.17530938907856217\n",
      "0.17522620791200264\n",
      "0.17514310339964534\n",
      "0.17506007543291957\n",
      "0.17497712390347117\n",
      "0.17489424870316173\n",
      "0.17481144972406737\n",
      "0.17472872685847943\n",
      "0.1746460799989029\n",
      "0.174563509038056\n",
      "0.17448101386887013\n",
      "0.17439859438448874\n",
      "Cost after iteration 1400: 0.174399\n",
      "0.17431625047826715\n",
      "0.17423398204377183\n",
      "0.17415178897477968\n",
      "0.17406967116527797\n",
      "0.17398762850946337\n",
      "0.17390566090174153\n",
      "0.1738237682367266\n",
      "0.17374195040924045\n",
      "0.1736602073143125\n",
      "0.17357853884717891\n",
      "0.17349694490328205\n",
      "0.17341542537827007\n",
      "0.1733339801679965\n",
      "0.17325260916851923\n",
      "0.17317131227610044\n",
      "0.1730900893872061\n",
      "0.1730089403985049\n",
      "0.17292786520686837\n",
      "0.17284686370937002\n",
      "0.1727659358032849\n",
      "0.17268508138608882\n",
      "0.1726043003554584\n",
      "0.17252359260927005\n",
      "0.17244295804559961\n",
      "0.17236239656272173\n",
      "0.1722819080591099\n",
      "0.17220149243343502\n",
      "0.17212114958456567\n",
      "0.17204087941156715\n",
      "0.17196068181370142\n",
      "0.1718805566904258\n",
      "0.17180050394139376\n",
      "0.17172052346645292\n",
      "0.17164061516564552\n",
      "0.17156077893920788\n",
      "0.17148101468756952\n",
      "0.1714013223113527\n",
      "0.17132170171137243\n",
      "0.17124215278863514\n",
      "0.17116267544433936\n",
      "0.17108326957987385\n",
      "0.17100393509681816\n",
      "0.1709246718969418\n",
      "0.17084547988220355\n",
      "0.1707663589547514\n",
      "0.17068730901692175\n",
      "0.17060832997123898\n",
      "0.17052942172041513\n",
      "0.17045058416734915\n",
      "0.17037181721512668\n",
      "0.17029312076701963\n",
      "0.17021449472648523\n",
      "0.17013593899716623\n",
      "0.17005745348288986\n",
      "0.16997903808766776\n",
      "0.1699006927156951\n",
      "0.16982241727135078\n",
      "0.16974421165919615\n",
      "0.1696660757839752\n",
      "0.16958800955061365\n",
      "0.16951001286421896\n",
      "0.16943208563007936\n",
      "0.16935422775366377\n",
      "0.16927643914062118\n",
      "0.16919871969678033\n",
      "0.16912106932814905\n",
      "0.16904348794091378\n",
      "0.16896597544143963\n",
      "0.16888853173626944\n",
      "0.16881115673212307\n",
      "0.16873385033589816\n",
      "0.16865661245466806\n",
      "0.16857944299568248\n",
      "0.16850234186636703\n",
      "0.16842530897432229\n",
      "0.16834834422732364\n",
      "0.16827144753332074\n",
      "0.16819461880043723\n",
      "0.1681178579369702\n",
      "0.16804116485138984\n",
      "0.16796453945233866\n",
      "0.16788798164863172\n",
      "0.1678114913492556\n",
      "0.1677350684633682\n",
      "0.16765871290029838\n",
      "0.16758242456954553\n",
      "0.1675062033807788\n",
      "0.16743004924383736\n",
      "0.1673539620687293\n",
      "0.16727794176563157\n",
      "0.16720198824488947\n",
      "0.16712610141701634\n",
      "0.1670502811926931\n",
      "0.16697452748276737\n",
      "0.1668988401982541\n",
      "0.16682321925033405\n",
      "0.16674766455035422\n",
      "0.16667217600982676\n",
      "0.16659675354042905\n",
      "0.16652139705400326\n",
      "Cost after iteration 1500: 0.166521\n",
      "0.16644610646255575\n",
      "0.16637088167825653\n",
      "0.16629572261343933\n",
      "0.16622062918060085\n",
      "0.16614560129240039\n",
      "0.16607063886165946\n",
      "0.16599574180136165\n",
      "0.16592091002465184\n",
      "0.165846143444836\n",
      "0.16577144197538082\n",
      "0.16569680552991314\n",
      "0.16562223402222\n",
      "0.1655477273662475\n",
      "0.16547328547610118\n",
      "0.16539890826604525\n",
      "0.16532459565050203\n",
      "0.1652503475440521\n",
      "0.16517616386143358\n",
      "0.16510204451754137\n",
      "0.16502798942742777\n",
      "0.164953998506301\n",
      "0.16488007166952565\n",
      "0.1648062088326217\n",
      "0.16473240991126462\n",
      "0.1646586748212847\n",
      "0.1645850034786668\n",
      "0.16451139579954988\n",
      "0.16443785170022684\n",
      "0.1643643710971437\n",
      "0.16429095390689982\n",
      "0.16421760004624703\n",
      "0.16414430943208966\n",
      "0.16407108198148376\n",
      "0.16399791761163718\n",
      "0.16392481623990882\n",
      "0.16385177778380827\n",
      "0.16377880216099605\n",
      "0.1637058892892824\n",
      "0.16363303908662755\n",
      "0.1635602514711408\n",
      "0.16348752636108102\n",
      "0.16341486367485525\n",
      "0.16334226333101917\n",
      "0.16326972524827627\n",
      "0.16319724934547775\n",
      "0.16312483554162208\n",
      "0.16305248375585446\n",
      "0.1629801939074669\n",
      "0.16290796591589748\n",
      "0.16283579970073\n",
      "0.16276369518169423\n",
      "0.16269165227866456\n",
      "0.16261967091166066\n",
      "0.16254775100084617\n",
      "0.16247589246652952\n",
      "0.16240409522916222\n",
      "0.16233235920933978\n",
      "0.16226068432780053\n",
      "0.16218907050542558\n",
      "0.1621175176632385\n",
      "0.16204602572240503\n",
      "0.16197459460423275\n",
      "0.16190322423017034\n",
      "0.1618319145218077\n",
      "0.16176066540087555\n",
      "0.161689476789245\n",
      "0.1616183486089272\n",
      "0.161547280782073\n",
      "0.1614762732309729\n",
      "0.16140532587805617\n",
      "0.16133443864589112\n",
      "0.1612636114571844\n",
      "0.1611928442347807\n",
      "0.1611221369016625\n",
      "0.16105148938094999\n",
      "0.1609809015959001\n",
      "0.1609103734699069\n",
      "0.16083990492650083\n",
      "0.16076949588934858\n",
      "0.1606991462822525\n",
      "0.16062885602915072\n",
      "0.16055862505411636\n",
      "0.16048845328135772\n",
      "0.16041834063521748\n",
      "0.1603482870401726\n",
      "0.1602782924208341\n",
      "0.1602083567019466\n",
      "0.160138479808388\n",
      "0.16006866166516928\n",
      "0.15999890219743412\n",
      "0.15992920133045885\n",
      "0.15985955898965126\n",
      "0.1597899751005515\n",
      "0.15972044958883128\n",
      "0.15965098238029277\n",
      "0.15958157340086981\n",
      "0.15951222257662634\n",
      "0.15944292983375677\n",
      "0.15937369509858546\n",
      "0.15930451829756614\n",
      "Cost after iteration 1600: 0.159305\n",
      "0.15923539935728237\n",
      "0.15916633820444637\n",
      "0.15909733476589932\n",
      "0.15902838896861096\n",
      "0.15895950073967896\n",
      "0.15889067000632898\n",
      "0.15882189669591434\n",
      "0.15875318073591543\n",
      "0.15868452205393985\n",
      "0.15861592057772167\n",
      "0.15854737623512158\n",
      "0.15847888895412637\n",
      "0.15841045866284842\n",
      "0.15834208528952587\n",
      "0.15827376876252205\n",
      "0.15820550901032523\n",
      "0.1581373059615484\n",
      "0.1580691595449289\n",
      "0.15800106968932792\n",
      "0.15793303632373104\n",
      "0.1578650593772469\n",
      "0.1577971387791075\n",
      "0.15772927445866797\n",
      "0.1576614663454057\n",
      "0.1575937143689212\n",
      "0.1575260184589362\n",
      "0.15745837854529512\n",
      "0.15739079455796354\n",
      "0.15732326642702832\n",
      "0.15725579408269738\n",
      "0.15718837745529948\n",
      "0.1571210164752837\n",
      "0.1570537110732194\n",
      "0.1569864611797958\n",
      "0.1569192667258219\n",
      "0.15685212764222597\n",
      "0.15678504386005537\n",
      "0.15671801531047624\n",
      "0.15665104192477353\n",
      "0.1565841236343502\n",
      "0.1565172603707273\n",
      "0.15645045206554384\n",
      "0.15638369865055615\n",
      "0.1563170000576377\n",
      "0.1562503562187792\n",
      "0.15618376706608791\n",
      "0.15611723253178744\n",
      "0.15605075254821768\n",
      "0.15598432704783438\n",
      "0.15591795596320907\n",
      "0.1558516392270285\n",
      "0.1557853767720947\n",
      "0.15571916853132436\n",
      "0.15565301443774907\n",
      "0.1555869144245146\n",
      "0.15552086842488086\n",
      "0.15545487637222158\n",
      "0.15538893820002406\n",
      "0.15532305384188896\n",
      "0.15525722323153013\n",
      "0.15519144630277404\n",
      "0.15512572298955996\n",
      "0.15506005322593935\n",
      "0.15499443694607576\n",
      "0.15492887408424447\n",
      "0.15486336457483257\n",
      "0.15479790835233825\n",
      "0.1547325053513708\n",
      "0.15466715550665056\n",
      "0.1546018587530082\n",
      "0.15453661502538488\n",
      "0.1544714242588317\n",
      "0.15440628638850978\n",
      "0.15434120134968962\n",
      "0.15427616907775132\n",
      "0.15421118950818397\n",
      "0.15414626257658554\n",
      "0.15408138821866263\n",
      "0.1540165663702303\n",
      "0.15395179696721165\n",
      "0.1538870799456378\n",
      "0.15382241524164733\n",
      "0.15375780279148663\n",
      "0.1536932425315089\n",
      "0.1536287343981746\n",
      "0.15356427832805075\n",
      "0.15349987425781078\n",
      "0.15343552212423475\n",
      "0.15337122186420832\n",
      "0.153306973414723\n",
      "0.15324277671287617\n",
      "0.1531786316958701\n",
      "0.15311453830101251\n",
      "0.15305049646571575\n",
      "0.152986506127497\n",
      "0.15292256722397773\n",
      "0.1528586796928834\n",
      "0.1527948434720438\n",
      "0.15273105849939209\n",
      "0.15266732471296507\n",
      "Cost after iteration 1700: 0.152667\n",
      "0.15260364205090277\n",
      "0.15254001045144833\n",
      "0.15247642985294763\n",
      "0.15241290019384898\n",
      "0.1523494214127035\n",
      "0.15228599344816393\n",
      "0.15222261623898534\n",
      "0.15215928972402407\n",
      "0.15209601384223836\n",
      "0.1520327885326875\n",
      "0.1519696137345318\n",
      "0.15190648938703236\n",
      "0.15184341542955096\n",
      "0.15178039180154962\n",
      "0.15171741844259073\n",
      "0.15165449529233646\n",
      "0.1515916222905486\n",
      "0.15152879937708874\n",
      "0.15146602649191743\n",
      "0.1514033035750946\n",
      "0.1513406305667788\n",
      "0.1512780074072274\n",
      "0.15121543403679608\n",
      "0.1511529103959386\n",
      "0.1510904364252073\n",
      "0.15102801206525154\n",
      "0.15096563725681886\n",
      "0.15090331194075385\n",
      "0.15084103605799845\n",
      "0.1507788095495915\n",
      "0.15071663235666846\n",
      "0.15065450442046152\n",
      "0.15059242568229902\n",
      "0.15053039608360563\n",
      "0.15046841556590182\n",
      "0.15040648407080362\n",
      "0.15034460154002297\n",
      "0.15028276791536663\n",
      "0.15022098313873677\n",
      "0.15015924715213044\n",
      "0.15009755989763923\n",
      "0.15003592131744936\n",
      "0.14997433135384114\n",
      "0.14991278994918933\n",
      "0.14985129704596212\n",
      "0.14978985258672173\n",
      "0.14972845651412361\n",
      "0.14966710877091677\n",
      "0.14960580929994308\n",
      "0.14954455804413738\n",
      "0.1494833549465271\n",
      "0.1494221999502324\n",
      "0.14936109299846545\n",
      "0.1493000340345307\n",
      "0.14923902300182446\n",
      "0.14917805984383475\n",
      "0.149117144504141\n",
      "0.14905627692641413\n",
      "0.14899545705441608\n",
      "0.14893468483199976\n",
      "0.14887396020310875\n",
      "0.14881328311177722\n",
      "0.14875265350212982\n",
      "0.14869207131838125\n",
      "0.1486315365048359\n",
      "0.14857104900588858\n",
      "0.14851060876602318\n",
      "0.1484502157298131\n",
      "0.14838986984192112\n",
      "0.14832957104709887\n",
      "0.14826931929018689\n",
      "0.14820911451611446\n",
      "0.14814895666989913\n",
      "0.148088845696647\n",
      "0.1480287815415519\n",
      "0.14796876414989593\n",
      "0.1479087934670487\n",
      "0.14784886943846745\n",
      "0.14778899200969678\n",
      "0.1477291611263684\n",
      "0.14766937673420108\n",
      "0.1476096387790004\n",
      "0.14754994720665845\n",
      "0.14749030196315396\n",
      "0.14743070299455185\n",
      "0.14737115024700298\n",
      "0.14731164366674432\n",
      "0.1472521832000985\n",
      "0.14719276879347368\n",
      "0.1471334003933634\n",
      "0.14707407794634642\n",
      "0.14701480139908638\n",
      "0.14695557069833193\n",
      "0.14689638579091632\n",
      "0.14683724662375736\n",
      "0.14677815314385698\n",
      "0.14671910529830134\n",
      "0.1466601030342607\n",
      "0.14660114629898888\n",
      "0.14654223503982336\n",
      "Cost after iteration 1800: 0.146542\n",
      "0.1464833692041853\n",
      "0.14642454873957886\n",
      "0.14636577359359124\n",
      "0.1463070437138929\n",
      "0.14624835904823663\n",
      "0.1461897195444581\n",
      "0.1461311251504755\n",
      "0.14607257581428867\n",
      "0.14601407148398027\n",
      "0.1459556121077143\n",
      "0.14589719763373668\n",
      "0.14583882801037498\n",
      "0.14578050318603808\n",
      "0.14572222310921598\n",
      "0.14566398772848\n",
      "0.145605796992482\n",
      "0.14554765084995488\n",
      "0.1454895492497121\n",
      "0.14543149214064718\n",
      "0.1453734794717342\n",
      "0.14531551119202715\n",
      "0.14525758725066001\n",
      "0.14519970759684647\n",
      "0.1451418721798797\n",
      "0.14508408094913236\n",
      "0.1450263338540564\n",
      "0.14496863084418285\n",
      "0.1449109718691215\n",
      "0.14485335687856116\n",
      "0.14479578582226899\n",
      "0.14473825865009077\n",
      "0.14468077531195056\n",
      "0.14462333575785044\n",
      "0.14456593993787048\n",
      "0.14450858780216866\n",
      "0.1444512793009804\n",
      "0.14439401438461885\n",
      "0.14433679300347435\n",
      "0.1442796151080144\n",
      "0.14422248064878354\n",
      "0.14416538957640335\n",
      "0.14410834184157173\n",
      "0.14405133739506346\n",
      "0.14399437618772964\n",
      "0.14393745817049755\n",
      "0.14388058329437053\n",
      "0.14382375151042798\n",
      "0.14376696276982498\n",
      "0.1437102170237922\n",
      "0.14365351422363587\n",
      "0.14359685432073757\n",
      "0.143540237266554\n",
      "0.14348366301261667\n",
      "0.14342713151053224\n",
      "0.14337064271198213\n",
      "0.14331419656872202\n",
      "0.14325779303258215\n",
      "0.14320143205546723\n",
      "0.14314511358935572\n",
      "0.14308883758630023\n",
      "0.1430326039984273\n",
      "0.14297641277793674\n",
      "0.14292026387710247\n",
      "0.14286415724827126\n",
      "0.1428080928438634\n",
      "0.1427520706163721\n",
      "0.1426960905183637\n",
      "0.14264015250247702\n",
      "0.1425842565214239\n",
      "0.1425284025279883\n",
      "0.14247259047502672\n",
      "0.14241682031546796\n",
      "0.14236109200231273\n",
      "0.14230540548863368\n",
      "0.14224976072757517\n",
      "0.1421941576723536\n",
      "0.14213859627625614\n",
      "0.14208307649264199\n",
      "0.14202759827494113\n",
      "0.1419721615766548\n",
      "0.14191676635135506\n",
      "0.14186141255268486\n",
      "0.14180610013435768\n",
      "0.14175082905015748\n",
      "0.1416955992539388\n",
      "0.14164041069962613\n",
      "0.14158526334121424\n",
      "0.14153015713276768\n",
      "0.14147509202842087\n",
      "0.14142006798237805\n",
      "0.1413650849489127\n",
      "0.1413101428823679\n",
      "0.1412552417371559\n",
      "0.1412003814677581\n",
      "0.1411455620287249\n",
      "0.1410907833746754\n",
      "0.14103604546029772\n",
      "0.14098134824034814\n",
      "0.14092669166965172\n",
      "0.14087207570310153\n",
      "Cost after iteration 1900: 0.140872\n",
      "0.14081750029565926\n",
      "0.14076296540235406\n",
      "0.14070847097828337\n",
      "0.1406540169786124\n",
      "0.1405996033585736\n",
      "0.1405452300734676\n",
      "0.1404908970786616\n",
      "0.14043660432959082\n",
      "0.14038235178175684\n",
      "0.1403281393907288\n",
      "0.14027396711214246\n",
      "0.14021983490170023\n",
      "0.14016574271517113\n",
      "0.14011169050839073\n",
      "0.14005767823726079\n",
      "0.14000370585774946\n",
      "0.1399497733258906\n",
      "0.13989588059778432\n",
      "0.13984202762959647\n",
      "0.13978821437755842\n",
      "0.13973444079796737\n",
      "0.13968070684718573\n",
      "0.13962701248164108\n",
      "0.13957335765782652\n",
      "0.13951974233229994\n",
      "0.13946616646168417\n",
      "0.1394126300026669\n",
      "0.1393591329120004\n",
      "0.13930567514650155\n",
      "0.13925225666305158\n",
      "0.139198877418596\n",
      "0.13914553737014457\n",
      "0.1390922364747709\n",
      "0.13903897468961288\n",
      "0.13898575197187166\n",
      "0.13893256827881262\n",
      "0.1388794235677642\n",
      "0.13882631779611845\n",
      "0.13877325092133083\n",
      "0.13872022290091982\n",
      "0.13866723369246703\n",
      "0.13861428325361694\n",
      "0.13856137154207687\n",
      "0.1385084985156169\n",
      "0.1384556641320695\n",
      "0.1384028683493298\n",
      "0.13835011112535509\n",
      "0.13829739241816497\n",
      "0.138244712185841\n",
      "0.1381920703865269\n",
      "0.1381394669784281\n",
      "0.13808690191981168\n",
      "0.1380343751690067\n",
      "0.13798188668440328\n",
      "0.13792943642445316\n",
      "0.1378770243476692\n",
      "0.1378246504126256\n",
      "0.13777231457795738\n",
      "0.13772001680236054\n",
      "0.13766775704459194\n",
      "0.13761553526346904\n",
      "0.1375633514178699\n",
      "0.13751120546673287\n",
      "0.13745909736905695\n",
      "0.13740702708390104\n",
      "0.13735499457038428\n",
      "0.1373029997876858\n",
      "0.13725104269504454\n",
      "0.13719912325175931\n",
      "0.13714724141718837\n",
      "0.13709539715074975\n",
      "0.13704359041192057\n",
      "0.13699182116023767\n",
      "0.13694008935529672\n",
      "0.13688839495675267\n",
      "0.13683673792431936\n",
      "0.13678511821776948\n",
      "0.13673353579693448\n",
      "0.1366819906217045\n",
      "0.13663048265202804\n",
      "0.13657901184791219\n",
      "0.1365275781694222\n",
      "0.1364761815766817\n",
      "0.13642482202987205\n",
      "0.136373499489233\n",
      "0.1363222139150619\n",
      "0.13627096526771376\n",
      "0.13621975350760157\n",
      "0.13616857859519563\n",
      "0.1361174404910235\n",
      "0.13606633915567057\n",
      "0.1360152745497789\n",
      "0.13596424663404794\n",
      "0.13591325536923402\n",
      "0.1358623007161504\n",
      "0.13581138263566722\n",
      "0.13576050108871107\n",
      "0.13570965603626528\n",
      "0.1356588474393696\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!\n",
    "\n",
    "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1, you predicted that it is a \"cat\" picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Further analysis (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a model with learning rate: 0.01\n",
      "0.6931471805599453\n",
      "1.120173947456166\n",
      "2.11271838059857\n",
      "3.199729017798442\n",
      "0.8100261060105536\n",
      "1.8874109951783222\n",
      "3.1881676760526965\n",
      "0.7949783817423247\n",
      "1.870697417485222\n",
      "3.1513560070049493\n",
      "0.7648481658963865\n",
      "1.8022023250899095\n",
      "3.113051478606344\n",
      "0.7356117417067405\n",
      "1.7216011538215938\n",
      "3.0698415680999123\n",
      "0.7051661626075594\n",
      "1.6227657065309706\n",
      "3.0182244725746434\n",
      "0.671908180615312\n",
      "1.4963431351303793\n",
      "2.9490220595865533\n",
      "0.6323339800435085\n",
      "1.3139214953075364\n",
      "2.8283499427181233\n",
      "0.5775558543072072\n",
      "0.9667092700909942\n",
      "2.41687970284653\n",
      "0.5594224794650056\n",
      "0.8473694424360649\n",
      "1.8485074169447226\n",
      "2.9149952446403513\n",
      "0.60613985748882\n",
      "1.236423200116336\n",
      "2.6967900616880365\n",
      "0.5264808290812681\n",
      "0.6582804224087058\n",
      "1.5344517149021752\n",
      "1.2992095821532053\n",
      "2.7144409289813622\n",
      "0.5234222868466297\n",
      "0.7190704405345021\n",
      "1.7794478196160686\n",
      "0.9748252434435577\n",
      "2.3604361833632375\n",
      "0.5209805965829352\n",
      "0.7649435446897981\n",
      "1.6727340564222652\n",
      "2.78390760034261\n",
      "0.5362945103956762\n",
      "0.9241854522215884\n",
      "2.2459098104930644\n",
      "0.5440577551137478\n",
      "1.0007286223980496\n",
      "1.6729387540354395\n",
      "2.7317385325538677\n",
      "0.512874240688353\n",
      "0.8086484898923761\n",
      "1.9933781064357399\n",
      "0.6775869918656784\n",
      "1.6302372911432215\n",
      "1.018099432616857\n",
      "2.331495796020242\n",
      "0.4831980039369834\n",
      "0.6386608630816133\n",
      "1.3756595613542955\n",
      "2.591410221529216\n",
      "0.467784057349858\n",
      "0.5487660938455428\n",
      "1.1066184003326913\n",
      "1.494413692522072\n",
      "2.6037836476421043\n",
      "0.4653309469241913\n",
      "0.5768729292054363\n",
      "1.2526730641553996\n",
      "1.3333306465970525\n",
      "2.5157589972523127\n",
      "0.4472538807599756\n",
      "0.4698706978926018\n",
      "0.6675236462719032\n",
      "1.3962846965452276\n",
      "2.5213093831244073\n",
      "0.44193488465630676\n",
      "0.47668087099918255\n",
      "0.7508685148429765\n",
      "1.4747004584530972\n",
      "2.522443200314282\n",
      "0.4369847569007003\n",
      "0.4810024640550315\n",
      "0.8047960008837897\n",
      "1.4787296278103634\n",
      "2.498016227554142\n",
      "0.4292473733836485\n",
      "0.45970402501784446\n",
      "0.6942721298669519\n",
      "1.3852781409827615\n",
      "2.4459690343126868\n",
      "0.41973513124327316\n",
      "0.4249597265107082\n",
      "0.4793408810209075\n",
      "0.8239208681637452\n",
      "1.9230473939968418\n",
      "0.5781319728474138\n",
      "1.2919594897086586\n",
      "1.1420299086909664\n",
      "2.282152486226133\n",
      "0.4167830719457346\n",
      "0.4692328445959175\n",
      "0.788542568679071\n",
      "1.836225604582383\n",
      "0.6139244199513735\n",
      "1.4055609737952415\n",
      "0.9903019150510937\n",
      "2.112898294522723\n",
      "0.44762208280829\n",
      "0.7050147714840065\n",
      "1.3112302672654246\n",
      "2.33172032615072\n",
      "0.39761553078653167\n",
      "0.39981627931374436\n",
      "0.42599308477838277\n",
      "0.607911852335522\n",
      "1.1893062920660236\n",
      "2.2512693635397265\n",
      "0.3971788194521494\n",
      "0.4384844380550091\n",
      "0.6812404541570442\n",
      "1.574010670817376\n",
      "0.7709775938305103\n",
      "1.7546247360529073\n",
      "0.6137229927489554\n",
      "1.3844924412122535\n",
      "0.9240318146361967\n",
      "1.970732383321585\n",
      "0.46861625112815425\n",
      "0.8560889169875899\n",
      "1.2654498478200251\n",
      "2.233261248876649\n",
      "0.38386542867974766\n",
      "0.41005674659887986\n",
      "0.5626523034495893\n",
      "1.2179056109719089\n",
      "1.0295566085686254\n",
      "2.0489708422927384\n",
      "0.41823995458557006\n",
      "0.6286630346811668\n",
      "1.136655161866728\n",
      "2.1195655374370737\n",
      "0.39189942183757087\n",
      "0.49688448670583724\n",
      "0.8895879052667929\n",
      "1.866943476265855\n",
      "0.4843103110839282\n",
      "0.9324812962661158\n",
      "1.1647948186590058\n",
      "2.1062344185469803\n",
      "0.3847853138939805\n",
      "0.4796140988894077\n",
      "0.8324501266828289\n",
      "1.7610680042035902\n",
      "0.5276028208543284\n",
      "1.0800949124386532\n",
      "1.0461902711856272\n",
      "1.9869885834072796\n",
      "0.4084255319823273\n",
      "0.6130529006667248\n",
      "1.0527619598541655\n",
      "1.9777671181553869\n",
      "0.4056473841502475\n",
      "0.6060149688736318\n",
      "1.0333384568294062\n",
      "1.9466009189762719\n",
      "0.4108828708851758\n",
      "0.6344882510871012\n",
      "1.048373954163528\n",
      "1.944760685962089\n",
      "0.40547576985347467\n",
      "0.6148501699467906\n",
      "1.0182560968335237\n",
      "1.9043998866683653\n",
      "0.4147187621624351\n",
      "0.6574978701435729\n",
      "1.039063581576135\n",
      "1.907946651481484\n",
      "0.4071378843039198\n",
      "0.6285467481465739\n",
      "1.0052127983819061\n",
      "1.8641186671625056\n",
      "0.4184030049659823\n",
      "0.6757951669110681\n",
      "1.0221808370781977\n",
      "1.8649126172244668\n",
      "0.41174135184938665\n",
      "0.650742770618012\n",
      "0.9948703730247032\n",
      "1.8264673937853975\n",
      "0.4214459352822748\n",
      "0.6882636795624271\n",
      "1.0001075813899765\n",
      "1.8172203683105308\n",
      "0.41894456444374967\n",
      "0.6787419985263368\n",
      "0.9824922386675682\n",
      "1.7871399633804401\n",
      "0.42559192946097457\n",
      "0.7019160774251221\n",
      "0.9771468472088581\n",
      "1.7683291497265698\n",
      "0.4274099063777299\n",
      "0.7068121143304507\n",
      "0.9647034770532643\n",
      "1.7427837660207568\n",
      "0.43240682277385045\n",
      "0.7218030250483327\n",
      "0.9543056255158967\n",
      "1.719089920975196\n",
      "0.4366947636525637\n",
      "0.7333698576632219\n",
      "0.9423630318433489\n",
      "1.693766474533182\n",
      "0.4418898908049326\n",
      "0.7468998326574523\n",
      "0.9300081915858239\n",
      "1.6678722771869698\n",
      "0.44750006505999784\n",
      "0.7607015327927933\n",
      "0.9169542430040447\n",
      "1.6410723737911752\n",
      "0.45369233280793514\n",
      "0.7752342495011321\n",
      "0.9032377691501321\n",
      "1.6133485420034386\n",
      "0.4604810489056493\n",
      "0.7904602697535423\n",
      "0.8888416826107902\n",
      "1.5846225927693307\n",
      "0.4679089764512728\n",
      "0.806421742037377\n",
      "0.8737542555862378\n",
      "1.554816001993101\n",
      "0.476017225530411\n",
      "0.8231469632361077\n",
      "0.8579659271360152\n",
      "1.5238467347342755\n",
      "0.48484549263570326\n",
      "0.8406522662627878\n",
      "0.841473054246558\n",
      "1.4916340618150623\n",
      "0.4944276037860344\n",
      "0.8589317964226636\n",
      "0.8242836711369489\n",
      "1.4581069885513271\n",
      "0.5047842412199389\n",
      "0.877942572631388\n",
      "0.8064263226982641\n",
      "1.4232185275494234\n",
      "0.515911428881623\n",
      "0.8975829869497196\n",
      "0.7879629343274077\n",
      "1.3869684289834991\n",
      "0.5277633171065802\n",
      "0.9176636140067446\n",
      "0.769006317464136\n",
      "1.3494369667979436\n",
      "0.5402283665732189\n",
      "0.9378710640878237\n",
      "0.7497416186111426\n",
      "1.3108307097513805\n",
      "0.5531001512675381\n",
      "0.9577300216948826\n",
      "0.7304481127213914\n",
      "1.2715360793733943\n",
      "0.5660488030485776\n",
      "0.9765764699845477\n",
      "0.7115128558845268\n",
      "1.2321661458132016\n",
      "0.5786068765123235\n",
      "0.9935651335981138\n",
      "0.6934220895667849\n",
      "1.1935716038565718\n",
      "0.590190957390618\n",
      "1.0077397587713457\n",
      "0.6767145065973517\n",
      "1.156777078322515\n",
      "0.6001785611854664\n",
      "1.018183083763764\n",
      "0.6618900852504251\n",
      "1.1228172864515942\n",
      "0.6080376244218814\n",
      "1.02422418877408\n",
      "0.6492929237484677\n",
      "1.0924984907750483\n",
      "0.6134660681151246\n",
      "1.0256280699565916\n",
      "0.6390147798342571\n",
      "1.0661789467072764\n",
      "0.6164722562777246\n",
      "1.022671171182149\n",
      "0.6308709312791446\n",
      "1.043685068866471\n",
      "0.6173497038063914\n",
      "1.016054845894413\n",
      "0.6244656437405925\n",
      "1.0244129952484453\n",
      "0.6165622927558517\n",
      "1.0066994956501478\n",
      "0.6193133965450044\n",
      "1.007553428028869\n",
      "0.6146044439374075\n",
      "0.9955202305377848\n",
      "0.6149567915319833\n",
      "0.992319165212625\n",
      "0.6118972976477666\n",
      "0.9832705210173727\n",
      "0.6110383618354195\n",
      "0.9780867649602397\n",
      "0.6087443769350823\n",
      "0.9704829813552174\n",
      "0.6073186195045014\n",
      "0.9644358480366456\n",
      "0.6053357669733505\n",
      "0.957486491431803\n",
      "0.6036578286699749\n",
      "0.9511196402462416\n",
      "0.6017764405242634\n",
      "0.9444598783860695\n",
      "0.5999845679383449\n",
      "0.9380104224703689\n",
      "0.5981180755434958\n",
      "0.9314889108459117\n",
      "0.5962670071503148\n",
      "0.9250493677159115\n",
      "0.5943836971909318\n",
      "0.9186096430317404\n",
      "0.5924931062320872\n",
      "0.9122120212420026\n",
      "0.5905829225479358\n",
      "0.9058349975029407\n",
      "0.5886594198683255\n",
      "0.8994888491535306\n",
      "0.5867198606358104\n",
      "0.8931684651658426\n",
      "0.5847657381682301\n",
      "0.8868759581056026\n",
      "0.5827966104050223\n",
      "0.880610144988698\n",
      "0.5808128836880602\n",
      "0.8743712914246785\n",
      "0.5788145954187373\n",
      "0.8681590389126368\n",
      "0.576801929074856\n",
      "0.8619732808085792\n",
      "0.5747750035304846\n",
      "0.8558138042796263\n",
      "0.5727339561543435\n",
      "0.8496804313680101\n",
      "0.5706789110863905\n",
      "0.8435729651913836\n",
      "0.5686099907578069\n",
      "0.8374912094894873\n",
      "0.5665273117085874\n",
      "0.8314349615889763\n",
      "0.5644309858642228\n",
      "0.8254040146272604\n",
      "0.562321119969368\n",
      "0.8193981566789134\n",
      "0.5601978155980686\n",
      "0.8134171708746974\n",
      "0.5580611689743034\n",
      "0.807460835218374\n",
      "0.5559112708373355\n",
      "0.8015289224962342\n",
      "0.5537482062862573\n",
      "0.7956212001677972\n",
      "0.5515720546244256\n",
      "0.7897374302714629\n",
      "0.5493828892012015\n",
      "0.7838773693395357\n",
      "0.5471807772546436\n",
      "0.7780407683274082\n",
      "0.5449657797574516\n",
      "0.7722273725592629\n",
      "0.5427379512688705\n",
      "0.7664369216932598\n",
      "0.5404973397952396\n",
      "0.7606691497090103\n",
      "0.5382439866619113\n",
      "0.754923784920121\n",
      "0.5359779263992249\n",
      "0.7492005500144658\n",
      "0.5336991866451734\n",
      "0.7434991621246841\n",
      "0.5314077880673207\n",
      "0.7378193329312573\n",
      "0.5291037443064217\n",
      "0.7321607688002786\n",
      "0.5267870619440537\n",
      "0.7265231709578197\n",
      "0.5244577404964321\n",
      "0.7209062357025425\n",
      "0.5221157724364025\n",
      "0.7153096546579427\n",
      "0.5197611432454227\n",
      "0.7097331150653396\n",
      "0.5173938314971553\n",
      "0.7041763001184523\n",
      "0.5150138089740851\n",
      "0.6986388893401089\n",
      "0.5126210408183729\n",
      "0.6931205590013837\n",
      "0.5102154857179494\n",
      "0.6876209825831534\n",
      "0.5077970961286334\n",
      "0.6821398312798251\n",
      "0.5053658185328695\n",
      "0.6766767745447242\n",
      "0.5029215937354633\n",
      "0.6712314806763897\n",
      "0.5004643571965062\n",
      "0.6658036174448047\n",
      "0.4979940394014948\n",
      "0.6603928527563779\n",
      "0.495510566268474\n",
      "0.6549988553563019\n",
      "0.4930138595918636\n",
      "0.6496212955667434\n",
      "0.4905038375224851\n",
      "0.6442598460591654\n",
      "0.48798041508315315\n",
      "0.638914182658941\n",
      "0.48544350471908015\n",
      "0.6335839851803131\n",
      "0.4828930168822231\n",
      "0.6282689382896515\n",
      "0.48032886064859887\n",
      "0.6229687323948595\n",
      "0.4777509443675195\n",
      "0.6176830645587627\n",
      "0.4751591763416083\n",
      "0.6124116394342084\n",
      "0.4725534655364094\n",
      "0.6071541702186125\n",
      "0.4699337223183479\n",
      "0.6019103796256481\n",
      "0.4672998592197583\n",
      "0.5966800008717659\n",
      "0.46465179172967\n",
      "0.5914627786752356\n",
      "0.4619894391090196\n",
      "0.5862584702654007\n",
      "0.45931272522894007\n",
      "0.58106684639986\n",
      "0.45662157943077386\n",
      "0.5758876923872951\n",
      "0.45391593740645036\n",
      "0.5707208091136957\n",
      "0.45119574209785884\n",
      "0.5655660140697347\n",
      "0.44846094461385344\n",
      "0.5604231423770708\n",
      "0.44571150516351216\n",
      "0.555292047811364\n",
      "0.4429473940042571\n",
      "0.5501726038197664\n",
      "0.4401685924034303\n",
      "0.5450647045306732\n",
      "0.4373750936118763\n",
      "0.5399682657534393\n",
      "0.43456690384803937\n",
      "0.5348832259657541\n",
      "0.4317440432910113\n",
      "0.5298095472862705\n",
      "0.42890654708088066\n",
      "0.5247472164299819\n",
      "0.4260544663246152\n",
      "0.5196962456437123\n",
      "0.4231878691055577\n",
      "0.5146566736189065\n",
      "0.420306841494445\n",
      "0.5096285663787024\n",
      "0.41741148855962934\n",
      "0.5046120181360068\n",
      "0.414501935373926\n",
      "0.499607152119012\n",
      "0.4115783280152062\n",
      "0.4946141213602239\n",
      "0.4086408345574852\n",
      "0.4896331094446946\n",
      "0.4056896460488751\n",
      "0.48466433121269725\n",
      "0.40272497747228575\n",
      "0.47970803341158713\n",
      "0.39974706868426935\n",
      "0.47476449529105463\n",
      "0.39675618532682827\n",
      "0.46983402913538974\n",
      "0.39375261970640824\n",
      "0.4649169807257598\n",
      "0.3907366916336353\n",
      "0.4600137297248619\n",
      "0.38770874921667303\n",
      "0.4551246899756455\n",
      "0.38466916960035474\n",
      "0.450250309705152\n",
      "0.3816183596425173\n",
      "0.4453910716238824\n",
      "0.3785567565182348\n",
      "0.44054749291050693\n",
      "0.3754848282419335\n",
      "0.4357201250712138\n",
      "0.3724030740967146\n",
      "0.4309095536625743\n",
      "0.369312024959606\n",
      "0.4261163978665078\n",
      "0.36621224351097686\n",
      "0.4213413099058276\n",
      "0.36310432431598105\n",
      "0.416584974288933\n",
      "0.3599888937657116\n",
      "0.4118481068725769\n",
      "0.3568666098657694\n",
      "0.40713145373229354\n",
      "0.3537381618602269\n",
      "0.4024357898310546\n",
      "0.35060426967953656\n",
      "0.39776191747812395\n",
      "0.34746568320185495\n",
      "0.3931106645718815\n",
      "0.34432318131853634\n",
      "0.3884828826226482\n",
      "0.341177570796266\n",
      "0.3838794445543205\n",
      "0.338029684930458\n",
      "0.379301242286849\n",
      "0.3348803819871678\n",
      "0.37474918410537544\n",
      "0.33173054343388686\n",
      "0.37022419182607536\n",
      "0.3285810719631692\n",
      "0.3657271977734583\n",
      "0.325432889317105\n",
      "0.3612591415889863\n",
      "0.3222869339251336\n",
      "0.35682096689630405\n",
      "0.3191441583725639\n",
      "0.3524136178540455\n",
      "0.31600552672231863\n",
      "0.3480380356329563\n",
      "0.3128720117177936\n",
      "0.34369515485981017\n",
      "0.3097445919001774\n",
      "0.3393859000761463\n",
      "0.3066242486789573\n",
      "0.33511118226499126\n",
      "0.303511963399535\n",
      "0.33087189550330987\n",
      "0.30040871445664674\n",
      "0.32666891380167457\n",
      "0.2973154745065053\n",
      "0.32250308819540846\n",
      "0.29423320783401075\n",
      "0.31837524415297086\n",
      "0.291162867933839\n",
      "0.3142861793674485\n",
      "0.2881053953655135\n",
      "0.3102366619954864\n",
      "0.2850617159424946\n",
      "0.3062274294046811\n",
      "0.282032739313745\n",
      "0.30225918748523023\n",
      "0.27901935799294025\n",
      "0.29833261057437616\n",
      "0.2760224468854405\n",
      "0.2944483420328631\n",
      "0.2730428633561543\n",
      "0.2906069955012042\n",
      "0.27008144787250216\n",
      "0.2868091568500831\n",
      "0.26713902524579314\n",
      "0.2830553868237893\n",
      "0.2642164064814685\n",
      "0.2793462243583226\n",
      "0.2613143912339518\n",
      "0.27568219053695286\n",
      "0.25843377084537833\n",
      "0.2720637931258094\n",
      "0.2555753319294504\n",
      "0.2684915316108932\n",
      "0.2527398604423552\n",
      "0.26496590263612413\n",
      "0.24992814616238976\n",
      "0.26148740572021173\n",
      "0.2471409874791127\n",
      "0.2580565491088206\n",
      "0.24437919637198421\n",
      "0.25467385559841754\n",
      "0.24164360343818175\n",
      "0.2513398681501277\n",
      "0.23893506281033444\n",
      "0.2480551550967624\n",
      "0.23625445678813606\n",
      "0.24482031473492374\n",
      "0.23360269999412842\n",
      "0.2416359790877617\n",
      "0.2309807428544359\n",
      "0.23850281662364833\n",
      "0.22838957420096678\n",
      "0.23542153372277813\n",
      "0.22583022279369072\n",
      "0.23239287469848502\n",
      "0.2233037575710938\n",
      "0.22941762020364073\n",
      "0.22081128645471135\n",
      "0.22649658388538293\n",
      "0.21835395356041465\n",
      "0.22363060719373218\n",
      "0.2159329347051877\n",
      "0.2208205523009969\n",
      "0.21354943114333602\n",
      "0.21806729314829412\n",
      "0.2112046615197123\n",
      "0.21537170470138273\n",
      "0.20889985208827053\n",
      "0.2127346505680494\n",
      "0.20663622531007564\n",
      "0.21015696920054616\n",
      "0.2044149870130954\n",
      "0.20763945897558672\n",
      "0.20223731236347142\n",
      "0.2051828625073362\n",
      "0.20010433096083238\n",
      "0.2027878506016887\n",
      "0.19801711142473136\n",
      "0.20045500629915805\n",
      "0.19597664588171462\n",
      "0.1981848094756086\n",
      "0.19398383478951142\n",
      "0.1959776224723613\n",
      "0.19203947254377135\n",
      "0.19383367720859354\n",
      "0.19014423430206373\n",
      "0.19175306418935253\n",
      "0.1882986644291401\n",
      "0.18973572376332756\n",
      "0.18650316691772764\n",
      "0.1877814399085125\n",
      "0.18475799807268561\n",
      "0.18588983673502685\n",
      "0.1830632616667254\n",
      "0.1840603777975059\n",
      "0.18141890668750302\n",
      "0.18229236821005262\n",
      "0.1798247277037375\n",
      "0.18058495946021821\n",
      "0.1782803677872364\n",
      "0.1789371567300097\n",
      "0.17678532384322074\n",
      "0.17734782845586422\n",
      "0.1753389541273569\n",
      "0.17581571779920513\n",
      "0.1739404876677474\n",
      "0.1743394556566933\n",
      "0.1725890352659416\n",
      "0.1729175748153573\n",
      "0.17128360172379245\n",
      "0.17154852485197258\n",
      "0.17002309893247727\n",
      "0.17023068738682098\n",
      "0.16880635946503708\n",
      "0.1689623913269168\n",
      "0.1676321503323263\n",
      "0.16774192777002647\n",
      "0.16649918659176652\n",
      "0.16656756428514802\n",
      "0.1654061445359084\n",
      "0.16543755833436868\n",
      "0.16435167423064348\n",
      "0.16435016965222882\n",
      "0.1633344112182544\n",
      "0.16330367144930524\n",
      "0.16235298724594335\n",
      "0.16229636035456857\n",
      "0.16140603992404334\n",
      "0.16132656505461945\n",
      "0.16049222125828264\n",
      "0.16039265362607164\n",
      "0.159610205036166\n",
      "0.1594930395895749\n",
      "0.15875869307813723\n",
      "0.1586261867400662\n",
      "0.15793642038945274\n",
      "0.1577906128279689\n",
      "0.15714215926866318\n",
      "0.15698489218062361\n",
      "0.15637472244358494\n",
      "0.1562076573628139\n",
      "0.155632965316075\n",
      "0.15545759998049125\n",
      "0.15491578740337017\n",
      "0.1547334707334241\n",
      "0.1542221330668034\n",
      "0.1540340788211822\n",
      "0.15355099161896907\n",
      "0.15335829080327076\n",
      "0.15290139689844803\n",
      "0.15270502900893465\n",
      "0.1522724263975422\n",
      "0.15207326958568115\n",
      "0.15166320002357273\n",
      "0.15146204026833607\n",
      "0.15107287856855314\n",
      "0.15087041794281875\n",
      "0.15050066195579256\n",
      "0.15029752607107302\n",
      "0.14994578732546926\n",
      "0.14974253203592228\n",
      "0.1494075270146525\n",
      "0.14920464445720272\n",
      "0.1488851864807902\n",
      "0.1486831105234648\n",
      "0.1483781022114368\n",
      "0.14817721337688775\n",
      "0.14788563965703894\n",
      "0.14768626958287334\n",
      "0.14740719121798637\n",
      "0.14720962671007973\n",
      "0.14694217431188536\n",
      "0.14674666104142708\n",
      "0.14649002954214296\n",
      "0.14629677543184816\n",
      "0.14605021898446122\n",
      "0.14585939732423453\n",
      "0.14562222460371582\n",
      "0.14543397693113966\n",
      "0.1452055468099369\n",
      "0.14501998558630314\n",
      "0.14479970315869028\n",
      "0.14461691426693984\n",
      "0.1444042271980771\n",
      "0.14422427228496798\n",
      "0.14401866746179837\n",
      "0.14384158614291886\n",
      "0.14364258660527476\n",
      "0.14346839854813911\n",
      "0.14327556067964126\n",
      "0.1431042675770795\n",
      "0.14291717853655475\n",
      "0.14274876597991532\n",
      "0.1425670413551465\n",
      "0.14240148061447916\n",
      "0.14222476228111455\n",
      "0.14206201199747745\n",
      "0.14188996616687682\n",
      "0.14172997396020598\n",
      "0.1415622894008879\n",
      "0.14140499339547283\n",
      "0.1412413798136533\n",
      "0.14108671008215312\n",
      "0.14092689664765023\n",
      "0.1407747765737545\n",
      "0.14061851057826996\n",
      "0.14046885813752663\n",
      "0.14031590377302564\n",
      "0.14016863273101318\n",
      "0.14001876997661097\n",
      "0.13987379100349456\n",
      "0.13972681460992822\n",
      "0.1395840363104835\n",
      "0.13943975487191962\n",
      "0.1392990847303171\n",
      "0.13915731983390722\n",
      "0.13901866507288532\n",
      "0.13887925051715805\n",
      "0.1387425188716608\n",
      "0.13860529994551316\n",
      "0.13847040035139244\n",
      "0.1383352331661321\n",
      "0.13820207636509668\n",
      "0.13806882723267733\n",
      "0.13793732629527986\n",
      "0.13780587114656917\n",
      "0.13767594191564317\n",
      "0.1375461657532552\n",
      "0.1374177272108151\n",
      "0.13728952359172908\n",
      "0.13716249815292045\n",
      "0.13703576869677797\n",
      "0.13691008243498326\n",
      "0.1367847363546088\n",
      "0.13666031916227211\n",
      "0.13653627281358272\n",
      "0.13641305850369786\n",
      "0.1362902349527505\n",
      "0.13616816130625675\n",
      "0.13604648991172175\n",
      "0.13592549867625864\n",
      "0.1358049146860925\n",
      "0.13568495153168714\n",
      "0.13556539569321113\n",
      "0.13544641013049913\n",
      "0.13532782831345705\n",
      "0.1352097735799805\n",
      "0.13509211641245747\n",
      "0.13497494933244555\n",
      "0.1348581718497726\n",
      "0.13474185267259453\n",
      "0.13462591397954526\n",
      "0.13451040620175034\n",
      "0.13439526914845518\n",
      "0.1342805393239799\n",
      "0.13416617019605165\n",
      "0.1340521877387964\n",
      "0.13393855596217344\n",
      "0.1338252929447452\n",
      "0.1337123708057315\n",
      "0.13359980175771496\n",
      "0.13348756413862856\n",
      "0.13337566584731042\n",
      "0.1332640899780588\n",
      "0.13315284129408902\n",
      "0.1330419065198702\n",
      "0.1329312881699119\n",
      "0.13282097573511048\n",
      "0.13271097014311953\n",
      "0.132601262991327\n",
      "0.13249185410971068\n",
      "0.1323827366996585\n",
      "0.13227390985120652\n",
      "0.1321653679882671\n",
      "0.13205710971942083\n",
      "0.1319491304022041\n",
      "0.13184142834794468\n",
      "0.13173399962940538\n",
      "0.13162684238978847\n",
      "0.1315199532521632\n",
      "0.13141333028031968\n",
      "0.1313069705231326\n",
      "0.13120087202437825\n",
      "0.1310950321646945\n",
      "0.13098944900625484\n",
      "0.13088412019031898\n",
      "0.13077904382107117\n",
      "0.13067421774644394\n",
      "0.1305696401260021\n",
      "0.13046530897330205\n",
      "0.13036122250972795\n",
      "0.13025737888309313\n",
      "0.13015377637848857\n",
      "0.13005041325389552\n",
      "0.12994728785713233\n",
      "0.1298443985377422\n",
      "0.12974174370359634\n",
      "0.12963932178134052\n",
      "0.12953713123532684\n",
      "0.12943517055799275\n",
      "0.12933343826623175\n",
      "0.12923193290936472\n",
      "0.12913065305285643\n",
      "0.1290295972958478\n",
      "0.12892876424857888\n",
      "0.12882815255437097\n",
      "0.12872776086473003\n",
      "0.12862758786262363\n",
      "0.12852763223765304\n",
      "0.12842789270876126\n",
      "0.12832836800082506\n",
      "0.1282290568657696\n",
      "0.128129958061266\n",
      "0.1280310703697621\n",
      "0.12793239257955613\n",
      "0.12783392350158132\n",
      "0.12773566195287614\n",
      "0.127637606771157\n",
      "0.12753975680056273\n",
      "0.12744211090415636\n",
      "0.1273446679517496\n",
      "0.12724742683052792\n",
      "0.12715038643472928\n",
      "0.12705354567460628\n",
      "0.12695690346772848\n",
      "0.12686045874649562\n",
      "0.12676421045084252\n",
      "0.1266681575345028\n",
      "0.12657229895891628\n",
      "0.1264766336984247\n",
      "0.1263811607351997\n",
      "0.126285879063535\n",
      "0.12619078768563396\n",
      "0.12609588561514168\n",
      "0.1260011718736541\n",
      "0.12590664549361213\n",
      "0.12581230551541514\n",
      "0.12571815098978203\n",
      "0.1256241809753675\n",
      "0.12553039454068113\n",
      "0.12543679076212094\n",
      "0.1253433687255251\n",
      "0.12525012752455164\n",
      "0.12515706626192735\n",
      "0.12506418404811298\n",
      "0.12497148000230235\n",
      "0.1248789532513227\n",
      "0.12478660293042958\n",
      "0.12469442818240037\n",
      "0.12460242815816133\n",
      "0.12451060201603996\n",
      "0.12441894892225541\n",
      "0.12432746805030043\n",
      "0.12423615858132095\n",
      "0.12414501970360382\n",
      "0.12405405061286696\n",
      "0.12396325051183262\n",
      "0.1238726186104457\n",
      "0.12378215412551713\n",
      "0.1236918562808836\n",
      "0.12360172430710854\n",
      "0.1235117574415946\n",
      "0.12342195492833169\n",
      "0.12333231601797182\n",
      "0.12324283996761465\n",
      "0.12315352604085314\n",
      "0.12306437350758961\n",
      "0.12297538164405777\n",
      "0.12288654973266408\n",
      "0.12279787706199101\n",
      "0.12270936292665874\n",
      "0.12262100662731432\n",
      "0.12253280747050928\n",
      "0.12244476476867856\n",
      "0.12235687784003083\n",
      "0.12226914600851899\n",
      "0.12218156860374198\n",
      "0.12209414496090826\n",
      "0.12200687442074717\n",
      "0.12191975632946758\n",
      "0.12183279003867606\n",
      "0.12174597490533291\n",
      "0.12165931029167637\n",
      "0.1215727955651757\n",
      "0.12148643009846083\n",
      "0.12140021326927443\n",
      "0.12131414446040506\n",
      "0.12122822305963876\n",
      "0.12114244845969575\n",
      "0.12105682005818158\n",
      "0.12097133725752703\n",
      "0.1208859994649391\n",
      "0.12080080609234396\n",
      "0.12071575655633786\n",
      "0.12063085027813243\n",
      "0.12054608668350647\n",
      "0.12046146520275271\n",
      "0.12037698527063086\n",
      "0.12029264632631632\n",
      "0.12020844781335317\n",
      "0.12012438917960538\n",
      "0.12004046987721052\n",
      "0.11995668936253227\n",
      "0.11987304709611524\n",
      "0.11978954254263889\n",
      "0.11970617517087341\n",
      "0.11962294445363468\n",
      "0.11953984986774147\n",
      "0.11945689089397166\n",
      "0.11937406701701996\n",
      "0.11929137772545592\n",
      "0.11920882251168251\n",
      "0.11912640087189494\n",
      "0.11904411230604052\n",
      "0.11896195631777841\n",
      "0.11887993241444067\n",
      "0.11879804010699284\n",
      "0.11871627890999605\n",
      "0.11863464834156852\n",
      "0.11855314792334892\n",
      "0.11847177718045841\n",
      "0.11839053564146478\n",
      "0.11830942283834642\n",
      "0.1182284383064564\n",
      "0.11814758158448731\n",
      "0.118066852214437\n",
      "0.1179862497415739\n",
      "0.117905773714403\n",
      "0.11782542368463314\n",
      "0.11774519920714323\n",
      "0.11766509983995006\n",
      "0.11758512514417588\n",
      "0.11750527468401685\n",
      "0.1174255480267116\n",
      "0.11734594474250977\n",
      "0.11726646440464203\n",
      "0.1171871065892893\n",
      "0.11710787087555324\n",
      "0.11702875684542642\n",
      "0.11694976408376313\n",
      "0.1168708921782512\n",
      "0.11679214071938263\n",
      "0.11671350930042625\n",
      "0.1166349975173993\n",
      "0.11655660496904066\n",
      "0.11647833125678345\n",
      "0.11640017598472802\n",
      "0.11632213875961589\n",
      "0.11624421919080363\n",
      "0.11616641689023677\n",
      "0.11608873147242435\n",
      "0.11601116255441415\n",
      "0.1159337097557673\n",
      "0.11585637269853395\n",
      "0.1157791510072287\n",
      "0.11570204430880722\n",
      "0.11562505223264187\n",
      "0.11554817441049854\n",
      "0.11547141047651359\n",
      "0.1153947600671708\n",
      "0.11531822282127885\n",
      "0.11524179837994898\n",
      "0.11516548638657292\n",
      "0.11508928648680089\n",
      "0.11501319832852042\n",
      "0.11493722156183482\n",
      "0.11486135583904196\n",
      "0.11478560081461378\n",
      "0.11470995614517565\n",
      "0.11463442148948585\n",
      "0.11455899650841563\n",
      "0.11448368086492947\n",
      "0.11440847422406528\n",
      "0.11433337625291494\n",
      "0.11425838662060551\n",
      "0.11418350499827994\n",
      "0.11410873105907833\n",
      "0.1140340644781196\n",
      "0.11395950493248319\n",
      "0.11388505210119085\n",
      "0.11381070566518876\n",
      "0.11373646530732999\n",
      "0.11366233071235707\n",
      "0.11358830156688417\n",
      "0.1135143775593811\n",
      "0.11344055838015517\n",
      "0.11336684372133544\n",
      "0.11329323327685586\n",
      "0.11321972674243899\n",
      "0.1131463238155798\n",
      "0.11307302419552982\n",
      "0.11299982758328163\n",
      "0.11292673368155247\n",
      "0.11285374219476972\n",
      "0.11278085282905538\n",
      "0.11270806529221064\n",
      "0.1126353792937013\n",
      "0.11256279454464319\n",
      "0.11249031075778726\n",
      "0.1124179276475053\n",
      "0.11234564492977571\n",
      "0.11227346232216956\n",
      "0.11220137954383626\n",
      "0.11212939631549011\n",
      "0.11205751235939675\n",
      "0.11198572739935923\n",
      "0.11191404116070529\n",
      "0.1118424533702735\n",
      "0.11177096375640092\n",
      "0.1116995720489095\n",
      "0.11162827797909412\n",
      "0.11155708127970917\n",
      "0.11148598168495658\n",
      "0.11141497893047339\n",
      "0.11134407275331931\n",
      "0.11127326289196504\n",
      "0.11120254908628005\n",
      "0.11113193107752067\n",
      "0.11106140860831863\n",
      "0.11099098142266957\n",
      "0.1109206492659211\n",
      "0.11085041188476215\n",
      "0.11078026902721133\n",
      "0.11071022044260605\n",
      "0.1106402658815912\n",
      "0.11057040509610902\n",
      "0.1105006378393876\n",
      "0.11043096386593065\n",
      "0.11036138293150724\n",
      "0.11029189479314072\n",
      "0.11022249920909925\n",
      "0.11015319593888494\n",
      "0.11008398474322421\n",
      "0.11001486538405778\n",
      "0.10994583762453042\n",
      "0.10987690122898196\n",
      "0.10980805596293666\n",
      "0.10973930159309464\n",
      "0.10967063788732186\n",
      "0.10960206461464081\n",
      "0.1095335815452214\n",
      "0.10946518845037169\n",
      "0.10939688510252894\n",
      "0.10932867127525064\n",
      "0.1092605467432053\n",
      "0.10919251128216424\n",
      "0.10912456466899242\n",
      "0.1090567066816401\n",
      "0.1089889370991341\n",
      "0.10892125570156957\n",
      "0.10885366227010154\n",
      "0.1087861565869367\n",
      "0.10871873843532523\n",
      "0.10865140759955247\n",
      "0.10858416386493153\n",
      "0.10851700701779457\n",
      "0.10844993684548551\n",
      "0.10838295313635206\n",
      "0.108316055679738\n",
      "0.10824924426597553\n",
      "0.10818251868637799\n",
      "0.10811587873323208\n",
      "0.10804932419979074\n",
      "0.1079828548802654\n",
      "0.10791647056981934\n",
      "0.10785017106455994\n",
      "0.10778395616153218\n",
      "0.10771782565871099\n",
      "0.10765177935499455\n",
      "0.10758581705019774\n",
      "0.10751993854504471\n",
      "0.10745414364116244\n",
      "0.10738843214107417\n",
      "0.10732280384819236\n",
      "0.10725725856681274\n",
      "0.1071917961021072\n",
      "0.10712641626011767\n",
      "0.10706111884774983\n",
      "0.10699590367276654\n",
      "0.10693077054378174\n",
      "0.1068657192702542\n",
      "0.10680074966248163\n",
      "0.10673586153159428\n",
      "0.10667105468954899\n",
      "0.10660632894912347\n",
      "0.1065416841239102\n",
      "0.10647712002831076\n",
      "0.10641263647752955\n",
      "0.10634823328756889\n",
      "0.10628391027522269\n",
      "0.10621966725807094\n",
      "0.10615550405447446\n",
      "0.10609142048356905\n",
      "0.1060274163652602\n",
      "0.10596349152021754\n",
      "0.10589964576986967\n",
      "0.10583587893639869\n",
      "0.10577219084273498\n",
      "0.10570858131255205\n",
      "0.10564505017026136\n",
      "0.1055815972410072\n",
      "0.10551822235066147\n",
      "0.1054549253258191\n",
      "0.10539170599379251\n",
      "0.10532856418260711\n",
      "0.10526549972099608\n",
      "0.10520251243839603\n",
      "0.10513960216494156\n",
      "0.10507676873146128\n",
      "0.10501401196947238\n",
      "0.10495133171117627\n",
      "0.10488872778945414\n",
      "0.10482620003786214\n",
      "0.10476374829062704\n",
      "0.10470137238264124\n",
      "0.10463907214945932\n",
      "0.10457684742729251\n",
      "0.10451469805300509\n",
      "0.10445262386410971\n",
      "0.10439062469876317\n",
      "0.10432870039576222\n",
      "0.10426685079453936\n",
      "0.1042050757351585\n",
      "0.10414337505831107\n",
      "0.10408174860531168\n",
      "0.10402019621809414\n",
      "0.10395871773920759\n",
      "0.10389731301181221\n",
      "0.10383598187967541\n",
      "0.10377472418716825\n",
      "0.1037135397792608\n",
      "0.10365242850151878\n",
      "0.10359139020009997\n",
      "0.1035304247217497\n",
      "0.10346953191379787\n",
      "0.10340871162415469\n",
      "0.10334796370130715\n",
      "0.10328728799431554\n",
      "0.1032266843528096\n",
      "0.10316615262698499\n",
      "0.10310569266759982\n",
      "0.103045304325971\n",
      "0.10298498745397086\n",
      "0.10292474190402348\n",
      "0.10286456752910154\n",
      "0.10280446418272259\n",
      "0.10274443171894608\n",
      "0.10268446999236948\n",
      "0.10262457885812551\n",
      "0.10256475817187832\n",
      "0.10250500778982086\n",
      "0.1024453275686708\n",
      "0.10238571736566812\n",
      "0.10232617703857157\n",
      "0.10226670644565541\n",
      "0.10220730544570648\n",
      "0.10214797389802109\n",
      "0.10208871166240188\n",
      "0.10202951859915449\n",
      "0.10197039456908535\n",
      "0.10191133943349757\n",
      "0.10185235305418903\n",
      "0.10179343529344866\n",
      "0.10173458601405382\n",
      "0.10167580507926757\n",
      "0.10161709235283545\n",
      "0.10155844769898274\n",
      "0.10149987098241178\n",
      "0.10144136206829914\n",
      "0.10138292082229239\n",
      "0.10132454711050809\n",
      "0.10126624079952862\n",
      "0.1012080017563992\n",
      "0.10114982984862593\n",
      "0.1010917249441724\n",
      "0.10103368691145757\n",
      "0.10097571561935265\n",
      "0.10091781093717903\n",
      "0.10085997273470526\n",
      "0.10080220088214462\n",
      "0.10074449525015294\n",
      "0.10068685570982529\n",
      "0.10062928213269422\n",
      "0.10057177439072694\n",
      "0.10051433235632287\n",
      "0.10045695590231137\n",
      "0.10039964490194903\n",
      "0.10034239922891766\n",
      "0.10028521875732119\n",
      "0.10022810336168443\n",
      "0.10017105291694962\n",
      "0.10011406729847476\n",
      "0.10005714638203111\n",
      "0.10000029004380077\n",
      "0.09994349816037465\n",
      "0.09988677060875\n",
      "0.0998301072663285\n",
      "0.09977350801091354\n",
      "0.09971697272070833\n",
      "0.09966050127431379\n",
      "0.09960409355072634\n",
      "0.0995477494293354\n",
      "0.09949146878992172\n",
      "0.09943525151265502\n",
      "0.09937909747809208\n",
      "0.09932300656717417\n",
      "0.09926697866122589\n",
      "0.09921101364195184\n",
      "0.09915511139143589\n",
      "0.09909927179213833\n",
      "0.09904349472689407\n",
      "0.0989877800789107\n",
      "0.09893212773176667\n",
      "0.09887653756940881\n",
      "0.09882100947615097\n",
      "0.09876554333667184\n",
      "0.09871013903601288\n",
      "0.09865479645957678\n",
      "0.09859951549312522\n",
      "0.09854429602277712\n",
      "0.09848913793500702\n",
      "0.09843404111664268\n",
      "0.09837900545486405\n",
      "0.09832403083720044\n",
      "0.09826911715152964\n",
      "0.09821426428607571\n",
      "0.09815947212940722\n",
      "0.09810474057043538\n",
      "0.09805006949841258\n",
      "0.09799545880293045\n",
      "0.09794090837391813\n",
      "0.09788641810164064\n",
      "0.09783198787669724\n",
      "0.09777761759001945\n",
      "0.09772330713286971\n",
      "0.09766905639683959\n",
      "0.09761486527384793\n",
      "0.09756073365613971\n",
      "0.09750666143628382\n",
      "0.09745264850717195\n",
      "0.09739869476201661\n",
      "0.09734480009434972\n",
      "0.09729096439802123\n",
      "0.09723718756719701\n",
      "0.09718346949635788\n",
      "0.09712981008029747\n",
      "0.09707620921412141\n",
      "0.0970226667932451\n",
      "0.09696918271339265\n",
      "0.0969157568705952\n",
      "0.0968623891611893\n",
      "0.09680907948181595\n",
      "0.09675582772941829\n",
      "0.09670263380124103\n",
      "0.0966494975948284\n",
      "0.09659641900802292\n",
      "0.09654339793896372\n",
      "0.09649043428608574\n",
      "0.09643752794811768\n",
      "0.09638467882408079\n",
      "0.09633188681328773\n",
      "0.09627915181534083\n",
      "0.09622647373013098\n",
      "0.096173852457836\n",
      "0.09612128789891958\n",
      "0.09606877995412982\n",
      "0.09601632852449796\n",
      "0.09596393351133674\n",
      "0.09591159481623954\n",
      "0.09585931234107878\n",
      "0.09580708598800468\n",
      "0.09575491565944404\n",
      "0.09570280125809895\n",
      "0.09565074268694528\n",
      "0.09559873984923191\n",
      "0.09554679264847894\n",
      "0.09549490098847709\n",
      "0.0954430647732853\n",
      "0.09539128390723112\n",
      "0.09533955829490807\n",
      "0.09528788784117524\n",
      "0.09523627245115572\n",
      "0.0951847120302357\n",
      "0.09513320648406287\n",
      "0.09508175571854552\n",
      "0.09503035963985158\n",
      "0.0949790181544069\n",
      "0.09492773116889452\n",
      "0.09487649859025346\n",
      "0.09482532032567724\n",
      "0.09477419628261338\n",
      "0.09472312636876162\n",
      "0.09467211049207316\n",
      "0.09462114856074974\n",
      "0.09457024048324178\n",
      "0.09451938616824829\n",
      "0.0944685855247149\n",
      "0.09441783846183341\n",
      "0.09436714488904012\n",
      "0.09431650471601549\n",
      "0.09426591785268233\n",
      "0.09421538420920543\n",
      "0.09416490369598983\n",
      "0.0941144762236803\n",
      "0.09406410170316018\n",
      "0.09401378004555024\n",
      "0.09396351116220762\n",
      "0.09391329496472503\n",
      "0.0938631313649297\n",
      "0.09381302027488199\n",
      "0.0937629616068751\n",
      "0.09371295527343343\n",
      "0.09366300118731198\n",
      "0.09361309926149514\n",
      "0.09356324940919587\n",
      "0.09351345154385472\n",
      "0.0934637055791389\n",
      "0.09341401142894105\n",
      "0.09336436900737875\n",
      "0.09331477822879321\n",
      "0.09326523900774841\n",
      "0.09321575125903053\n",
      "0.09316631489764628\n",
      "0.09311692983882286\n",
      "0.09306759599800621\n",
      "0.09301831329086091\n",
      "0.09296908163326849\n",
      "0.09291990094132722\n",
      "0.09287077113135074\n",
      "0.09282169211986747\n",
      "0.09277266382361951\n",
      "0.09272368615956206\n",
      "0.0926747590448621\n",
      "0.09262588239689802\n",
      "0.09257705613325835\n",
      "0.09252828017174129\n",
      "0.09247955443035354\n",
      "0.0924308788273096\n",
      "0.09238225328103096\n",
      "0.09233367771014517\n",
      "0.09228515203348504\n",
      "0.09223667617008809\n",
      "0.09218825003919504\n",
      "0.09213987356024982\n",
      "0.09209154665289822\n",
      "0.09204326923698732\n",
      "0.09199504123256463\n",
      "0.09194686255987719\n",
      "0.09189873313937107\n",
      "0.0918506528916903\n",
      "0.09180262173767607\n",
      "0.09175463959836633\n",
      "0.09170670639499444\n",
      "0.09165882204898913\n",
      "0.09161098648197302\n",
      "0.09156319961576229\n",
      "0.09151546137236588\n",
      "0.09146777167398462\n",
      "0.09142013044301056\n",
      "0.09137253760202615\n",
      "0.09132499307380379\n",
      "0.09127749678130459\n",
      "0.09123004864767814\n",
      "0.09118264859626155\n",
      "0.09113529655057874\n",
      "0.09108799243433978\n",
      "0.09104073617144011\n",
      "0.09099352768595995\n",
      "0.09094636690216346\n",
      "0.09089925374449806\n",
      "0.09085218813759391\n",
      "0.09080517000626302\n",
      "0.09075819927549868\n",
      "0.09071127587047464\n",
      "0.09066439971654468\n",
      "0.09061757073924162\n",
      "0.09057078886427687\n",
      "0.0905240540175398\n",
      "0.09047736612509683\n",
      "0.09043072511319099\n",
      "0.0903841309082412\n",
      "0.0903375834368416\n",
      "0.0902910826257609\n",
      "0.09024462840194179\n",
      "0.09019822069250029\n",
      "0.09015185942472487\n",
      "0.09010554452607623\n",
      "0.09005927592418651\n",
      "0.09001305354685844\n",
      "0.08996687732206493\n",
      "0.0899207471779485\n",
      "0.08987466304282048\n",
      "0.08982862484516053\n",
      "0.089782632513616\n",
      "0.08973668597700132\n",
      "0.08969078516429736\n",
      "0.08964493000465083\n",
      "0.08959912042737374\n",
      "0.08955335636194277\n",
      "0.08950763773799872\n",
      "0.08946196448534588\n",
      "0.08941633653395144\n",
      "0.0893707538139448\n",
      "0.08932521625561726\n",
      "0.0892797237894213\n",
      "0.08923427634596988\n",
      "0.0891888738560362\n",
      "0.0891435162505527\n",
      "0.0890982034606108\n",
      "0.08905293541746032\n",
      "0.08900771205250892\n",
      "0.08896253329732125\n",
      "0.088917399083619\n",
      "0.08887230934327948\n",
      "0.08882726400833613\n",
      "0.0887822630109771\n",
      "0.08873730628354527\n",
      "0.08869239375853716\n",
      "0.08864752536860306\n",
      "0.08860270104654593\n",
      "0.08855792072532116\n",
      "0.08851318433803615\n",
      "0.08846849181794934\n",
      "0.08842384309847014\n",
      "0.08837923811315823\n",
      "0.08833467679572306\n",
      "0.0882901590800232\n",
      "0.08824568490006622\n",
      "0.08820125419000764\n",
      "0.08815686688415093\n",
      "0.08811252291694667\n",
      "0.08806822222299224\n",
      "0.088023964737031\n",
      "0.08797975039395264\n",
      "0.08793557912879135\n",
      "0.08789145087672662\n",
      "0.08784736557308197\n",
      "0.08780332315332472\n",
      "0.08775932355306552\n",
      "0.08771536670805788\n",
      "0.08767145255419753\n",
      "0.0876275810275221\n",
      "0.08758375206421086\n",
      "0.08753996560058351\n",
      "0.08749622157310064\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Training a model with learning rate: 0.001\n",
      "0.6931471805599453\n",
      "0.6602163017690019\n",
      "0.6526585611009366\n",
      "0.6502497388185607\n",
      "0.6489887705026199\n",
      "0.6480063806213199\n",
      "0.6470998119666308\n",
      "0.6462198002970413\n",
      "0.6453540182894317\n",
      "0.644499232581576\n",
      "0.6436544764306668\n",
      "0.6428193536253203\n",
      "0.6419936145289407\n",
      "0.641177049817961\n",
      "0.6403694637979015\n",
      "0.6395706676604929\n",
      "0.6387804777293385\n",
      "0.6379987149485814\n",
      "0.6372252046793092\n",
      "0.6364597765717578\n",
      "0.6357022644565815\n",
      "0.6349525062417116\n",
      "0.6342103438117964\n",
      "0.633475622929652\n",
      "0.6327481931396676\n",
      "0.6320279076731887\n",
      "0.6313146233558969\n",
      "0.630608200517192\n",
      "0.6299085029015656\n",
      "0.6292153975819607\n",
      "0.6285287548750872\n",
      "0.627848448258686\n",
      "0.6271743542907063\n",
      "0.6265063525303773\n",
      "0.6258443254611479\n",
      "0.6251881584154632\n",
      "0.624537739501354\n",
      "0.623892959530807\n",
      "0.6232537119498838\n",
      "0.6226198927705644\n",
      "0.621991400504277\n",
      "0.621368136097089\n",
      "0.6207500028665225\n",
      "0.620136906439965\n",
      "0.6195287546946441\n",
      "0.6189254576991309\n",
      "0.6183269276563446\n",
      "0.6177330788480225\n",
      "0.617143827580626\n",
      "0.6165590921326486\n",
      "0.6159787927033\n",
      "0.6154028513625254\n",
      "0.6148311920023415\n",
      "0.6142637402894475\n",
      "0.6137004236190889\n",
      "0.6131411710701391\n",
      "0.6125859133613742\n",
      "0.6120345828089064\n",
      "0.6114871132847546\n",
      "0.6109434401765166\n",
      "0.6104035003481223\n",
      "0.6098672321016354\n",
      "0.6093345751400803\n",
      "0.6088054705312675\n",
      "0.6082798606725907\n",
      "0.6077576892567721\n",
      "0.6072389012385307\n",
      "0.6067234428021487\n",
      "0.6062112613299148\n",
      "0.6057023053714178\n",
      "0.6051965246136727\n",
      "0.6046938698520521\n",
      "0.6041942929620051\n",
      "0.6036977468715412\n",
      "0.6032041855344598\n",
      "0.6027135639043024\n",
      "0.6022258379090124\n",
      "0.6017409644262784\n",
      "0.6012589012595475\n",
      "0.6007796071146843\n",
      "0.6003030415772658\n",
      "0.5998291650904869\n",
      "0.5993579389336643\n",
      "0.5988893252013228\n",
      "0.5984232867828441\n",
      "0.5979597873426667\n",
      "0.5974987913010211\n",
      "0.5970402638151805\n",
      "0.5965841707612214\n",
      "0.596130478716272\n",
      "0.5956791549412395\n",
      "0.5952301673640018\n",
      "0.5947834845630494\n",
      "0.5943390757515676\n",
      "0.5938969107619441\n",
      "0.5934569600306918\n",
      "0.5930191945837756\n",
      "0.5925835860223293\n",
      "0.5921501065087554\n",
      "0.5917187287531943\n",
      "0.5912894260003538\n",
      "0.5908621720166888\n",
      "0.5904369410779213\n",
      "0.590013707956891\n",
      "0.5895924479117277\n",
      "0.5891731366743361\n",
      "0.5887557504391855\n",
      "0.5883402658523927\n",
      "0.5879266600010956\n",
      "0.587514910403102\n",
      "0.5871049949968137\n",
      "0.5866968921314115\n",
      "0.5862905805572991\n",
      "0.5858860394167947\n",
      "0.5854832482350667\n",
      "0.5850821869113033\n",
      "0.5846828357101138\n",
      "0.5842851752531508\n",
      "0.58388918651095\n",
      "0.5834948507949812\n",
      "0.5831021497499032\n",
      "0.5827110653460189\n",
      "0.5823215798719242\n",
      "0.5819336759273442\n",
      "0.5815473364161543\n",
      "0.5811625445395788\n",
      "0.5807792837895623\n",
      "0.5803975379423109\n",
      "0.580017291051996\n",
      "0.5796385274446189\n",
      "0.5792612317120299\n",
      "0.5788853887060984\n",
      "0.578510983533029\n",
      "0.5781380015478221\n",
      "0.577766428348872\n",
      "0.5773962497727007\n",
      "0.5770274518888225\n",
      "0.5766600209947379\n",
      "0.5762939436110506\n",
      "0.5759292064767055\n",
      "0.5755657965443468\n",
      "0.575203700975787\n",
      "0.574842907137591\n",
      "0.5744834025967663\n",
      "0.5741251751165605\n",
      "0.5737682126523604\n",
      "0.5734125033476926\n",
      "0.5730580355303191\n",
      "0.5727047977084311\n",
      "0.5723527785669316\n",
      "0.5720019669638106\n",
      "0.5716523519266059\n",
      "0.5713039226489504\n",
      "0.5709566684872008\n",
      "0.5706105789571488\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Training a model with learning rate: \" + str(lr))\n",
    "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Change your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the name of your image file\n",
    "my_image = \"my_image.jpg\"   \n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
    "\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember from this assignment:**\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
